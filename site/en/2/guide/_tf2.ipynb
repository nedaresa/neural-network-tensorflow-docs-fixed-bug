{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rUJv_fB-_w5Y"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\").\n",
        "\n",
        "# MNIST, with TensorFlow 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SjTFbdbu_w5a"
      },
      "source": [
        "## This notebook is still under construction! Please come back later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Abj4qkjT_w5b"
      },
      "source": [
        "This notebook trains a simple MNIST model, demonstrating a basic workflow using TensorFlow 2.0 APIs.\n",
        "\n",
        "The basic workflow consists of:\n",
        "\n",
        "- defining a model\n",
        "- preprocessing your data into a tf.data.Dataset\n",
        "- training over a dataset\n",
        "  - using tf.GradientTape to compute gradients\n",
        "  - using stateful tf.metrics.* APIs to record metrics of interest\n",
        "  - logging those metrics with tf.summary.* APIs so that they can be viewed in TensorBoard\n",
        "  - using tf.train.Checkpoint to save and restore weights\n",
        "- export a SavedModel using tf.saved_model (This SavedModel is a portable representation of the model, and can be imported into C++, JS, Python without knowledge of the original TensorFlow code.)\n",
        "- reimport that SavedModel and demonstrate its usage in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "q6Sd8USV_w5c"
      },
      "outputs": [],
      "source": [
        "!pip install tf-nightly-2.0-preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6Enr5rRN_w5g"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#TODO(brianklee): remove these when new modules are exported.\n",
        "from tensorflow.python.keras import metrics\n",
        "from tensorflow.python.ops import summary_ops_v2\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zMSdHwl5_w5k"
      },
      "outputs": [],
      "source": [
        "NUM_TRAIN_EPOCHS = 1\n",
        "# Where to save checkpoints, tensorboard summaries, etc.\n",
        "MODEL_DIR = '/tmp/tensorflow/mnist'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OT7IkP51_w5n"
      },
      "outputs": [],
      "source": [
        "# Define a convolution-based model, using Keras APIs.\n",
        "def create_model():\n",
        "  # Assumes data_format == 'channel_last'.\n",
        "  # See https://www.tensorflow.org/performance/performance_guide#data_formats\n",
        "\n",
        "  l = tf.keras.layers\n",
        "  max_pool = l.MaxPooling2D((2, 2), (2, 2), padding='same')\n",
        "  # The model consists of a sequential chain of layers, so tf.keras.Sequential\n",
        "  # (a subclass of tf.keras.Model) makes for a compact description.\n",
        "  model = tf.keras.Sequential(\n",
        "      [\n",
        "          l.Reshape(\n",
        "              target_shape=[28, 28, 1],\n",
        "              input_shape=(28, 28,)),\n",
        "          l.Conv2D(2, 5, padding='same', activation=tf.nn.relu),\n",
        "          max_pool,\n",
        "          l.Conv2D(4, 5, padding='same', activation=tf.nn.relu),\n",
        "          max_pool,\n",
        "          l.Flatten(),\n",
        "          l.Dense(32, activation=tf.nn.relu),\n",
        "          l.Dropout(0.4),\n",
        "          l.Dense(10)\n",
        "      ])\n",
        "  # TODO(brianklee): remove when Keras automatically wraps call/predict in tf.function calls.\n",
        "  model.call = tf.function(model.call)\n",
        "  return model\n",
        "\n",
        "# Define a loss function and accuracy function\n",
        "def compute_loss(logits, labels):\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "          logits=logits, labels=labels))\n",
        "\n",
        "\n",
        "def compute_accuracy(logits, labels):\n",
        "  predictions = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  return tf.reduce_mean(\n",
        "      tf.cast(tf.equal(predictions, labels), dtype=tf.float32))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Lr0eRymg_w5q"
      },
      "outputs": [],
      "source": [
        "# Set up datasets\n",
        "def mnist_datasets():\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  # Numpy defaults to dtype=float64; TF defaults to float32. Stick with float32.\n",
        "  x_train, x_test = x_train / np.float32(255), x_test / np.float32(255)\n",
        "  y_train, y_test = y_train.astype(np.int64), y_test.astype(np.int64)\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "  test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "  return train_dataset, test_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fC4MOuso_w5t"
      },
      "outputs": [],
      "source": [
        "# Define train/test routines\n",
        "\n",
        "# TODO(brianklee): Enable @tf.function on the training loop when zip, enumerate\n",
        "# are supported by autograph.\n",
        "def train(model, optimizer, dataset, global_step, num_steps=None):\n",
        "  \"\"\"Trains model on `dataset` using `optimizer`.\"\"\"\n",
        "  start = time.time()\n",
        "  avg_loss = metrics.Mean('loss', dtype=tf.float32)\n",
        "  avg_accuracy = metrics.Mean('accuracy', dtype=tf.float32)\n",
        "  for (batch, (images, labels)) in enumerate(dataset):\n",
        "    if num_steps is not None and batch \u003e num_steps:\n",
        "      break\n",
        "    # Record the operations used to compute the loss given the input,\n",
        "    # so that the gradient of the loss with respect to the variables\n",
        "    # can be computed.\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = model(images, training=True)\n",
        "      loss = compute_loss(logits, labels)\n",
        "      accuracy = compute_accuracy(logits, labels)\n",
        "    grads = tape.gradient(loss, model.variables)\n",
        "    optimizer.apply_gradients(\n",
        "        zip(grads, model.variables), global_step=global_step)\n",
        "    avg_loss(loss)\n",
        "    avg_accuracy(accuracy)\n",
        "    if batch % 10 == 0:\n",
        "      summary_ops_v2.scalar('loss', avg_loss.result(), step=global_step)\n",
        "      summary_ops_v2.scalar('accuracy', avg_accuracy.result(), step=global_step)\n",
        "      avg_loss.reset_states()\n",
        "      avg_accuracy.reset_states()\n",
        "      # TODO(brianklee): time.time() doesn't work in Graph mode. Would be nice to\n",
        "      # use a TF implementation of steps_per_sec.\n",
        "      rate = 10 / (time.time() - start)\n",
        "      print('Step #%d\\tLoss: %.6f (%d steps/sec)' % (batch, loss, rate))\n",
        "      start = time.time()\n",
        "\n",
        "\n",
        "def test(model, dataset, global_step):\n",
        "  \"\"\"Perform an evaluation of `model` on the examples from `dataset`.\"\"\"\n",
        "  avg_loss = metrics.Mean('loss', dtype=tf.float32)\n",
        "  avg_accuracy = metrics.Mean('accuracy', dtype=tf.float32)\n",
        "\n",
        "  for (images, labels) in dataset:\n",
        "    logits = model(images, training=False)\n",
        "    avg_loss(compute_loss(logits, labels))\n",
        "    avg_accuracy(compute_accuracy(logits, labels))\n",
        "  print('Model test set loss: {:0.4f} accuracy: {:0.2f}%'.format(\n",
        "      avg_loss.result(), avg_accuracy.result() * 100))\n",
        "  summary_ops_v2.scalar('loss', avg_loss.result(), step=global_step)\n",
        "  summary_ops_v2.scalar('accuracy', avg_accuracy.result(), step=global_step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "go-t0CvMzrSi"
      },
      "outputs": [],
      "source": [
        "def train_and_export():\n",
        "  \"\"\"Run MNIST training and eval loop in eager mode.\"\"\"\n",
        "  # Load the datasets\n",
        "  train_ds, test_ds = mnist_datasets()\n",
        "  train_ds = train_ds.shuffle(60000).batch(100)\n",
        "  test_ds = test_ds.batch(100)\n",
        "\n",
        "  # Create the model and optimizer\n",
        "  model = create_model()\n",
        "  optimizer = tf.train.MomentumOptimizer(0.01, 0.5)\n",
        "\n",
        "  # See summaries with `tensorboard --logdir=\u003cmodel_dir\u003e`\n",
        "  train_dir = os.path.join(MODEL_DIR, 'summaries', 'train')\n",
        "  test_dir = os.path.join(MODEL_DIR, 'summaries', 'eval')\n",
        "  summary_writer = summary_ops_v2.create_file_writer(\n",
        "      train_dir, flush_millis=10000)\n",
        "  test_summary_writer = summary_ops_v2.create_file_writer(\n",
        "      test_dir, flush_millis=10000, name='test')\n",
        "\n",
        "  # Create and restore checkpoint (if one exists on the path)\n",
        "  checkpoint_dir = os.path.join(MODEL_DIR, 'checkpoints')\n",
        "  checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
        "\n",
        "  global_step = tf.Variable(0, dtype=tf.int64)\n",
        "  checkpoint = tf.train.Checkpoint(\n",
        "      model=model, optimizer=optimizer, global_step=global_step)\n",
        "  # Restore variables on creation if a checkpoint exists.\n",
        "  checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "  # Train and evaluate for a set number of epochs.\n",
        "  for _ in range(NUM_TRAIN_EPOCHS):\n",
        "    start = time.time()\n",
        "    with summary_writer.as_default():\n",
        "      train(model, optimizer, train_ds, global_step)\n",
        "    end = time.time()\n",
        "    print('\\nTrain time for epoch #%d (%d total steps): %f' %\n",
        "          (checkpoint.save_counter.numpy() + 1,\n",
        "           global_step.numpy(),\n",
        "           end - start))\n",
        "    with test_summary_writer.as_default():\n",
        "      test(model, test_ds, global_step)\n",
        "    checkpoint.save(checkpoint_prefix)\n",
        "\n",
        "  export_path = os.path.join(MODEL_DIR, 'export')\n",
        "  tf.saved_model.save(\n",
        "      model, export_path, signatures=model.call.get_concrete_function(\n",
        "          tf.TensorSpec(shape=[None, 28, 28], dtype=tf.float32)))\n",
        "\n",
        "\n",
        "def import_and_eval():\n",
        "  export_path = os.path.join(MODEL_DIR, 'export')\n",
        "  model = tf.saved_model.restore(export_path)\n",
        "  _, (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  x_test = x_test / np.float32(255)\n",
        "  y_predict = model(x_test)\n",
        "  accuracy = compute_accuracy(y_predict, y_test)\n",
        "  print('Model accuracy: {:0.2f}%'.format(accuracy.numpy() * 100))\n",
        "\n",
        "# TODO(brianklee): Remove this after @allenl implements v2 import\n",
        "def temp_import_and_eval():\n",
        "  from tensorflow.python.saved_model import loader\n",
        "  from tensorflow.python.saved_model import tag_constants\n",
        "  export_path = os.path.join(MODEL_DIR, 'export')\n",
        "  _, (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  x_test = x_test / np.float32(255)\n",
        "  graph = tf.compat.v1.Graph()\n",
        "  with graph.as_default(), tf.compat.v1.Session(graph=graph) as session:\n",
        "    model = loader.load(session, [tag_constants.SERVING], export_path)\n",
        "    signature = model.signature_def['serving_default']\n",
        "    output_dict = {}\n",
        "    for output_name, output_tensor_info in signature.outputs.items():\n",
        "      output_dict[output_name] = graph.get_tensor_by_name(\n",
        "          output_tensor_info.name)\n",
        "    y_predict = session.run(output_dict['output_0'], feed_dict={'serving_default_inputs:0': x_test})\n",
        "  accuracy = compute_accuracy(y_predict, y_test)\n",
        "  print('Model accuracy: {:0.2f}%'.format(accuracy.numpy() * 100))\n",
        "\n",
        "\n",
        "def apply_clean():\n",
        "  if tf.io.gfile.exists(MODEL_DIR):\n",
        "    print('--clean flag set. Removing existing model dir: {}'.format(MODEL_DIR))\n",
        "    #TODO(brianklee): remove this hack after gfile migrations are done.\n",
        "    try:\n",
        "        tf.gfile.DeleteRecursively(MODEL_DIR)\n",
        "    except:\n",
        "        tf.io.gfile.rmtree(MODEL_DIR)\n",
        "\n",
        "\n",
        "apply_clean()\n",
        "train_and_export()\n",
        "# TODO(brianklee): Enable this functionality after @allenl implements this.\n",
        "# import_and_eval()\n",
        "temp_import_and_eval()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "TF2.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
