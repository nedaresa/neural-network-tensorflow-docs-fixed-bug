

page_type: reference


<!-- DO NOT EDIT! Automatically generated file. -->


# tf.contrib.bayesflow.csiszar_divergence.monte_carlo_csiszar_f_divergence

``` python
monte_carlo_csiszar_f_divergence(
    f,
    p,
    q,
    num_draws,
    use_reparametrization=True,
    seed=None,
    name=None
)
```



Defined in [`tensorflow/contrib/bayesflow/python/ops/csiszar_divergence_impl.py`](https://www.github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/bayesflow/python/ops/csiszar_divergence_impl.py).

Monte-Carlo approximation of the Csiszar f-Divergence.

A Csiszar-function is a member of,

```none
F = { f:R_+ to R : f convex }.
```

The Csiszar f-Divergence for Csiszar-function f is given by:

```none
D_f[p(X), q(X)] := E_{q(X)}[ f( p(X) / q(X) ) ]
                ~= m**-1 sum_j^m f( p(x_j) / q(x_j) ),
                           where x_j ~iid q(X)
```

Tricks: Reparameterization and Score-Gradient

When q is "reparameterized", i.e., a diffeomorphic transformation of a
parameterless distribution (e.g.,
`Normal(Y; m, s) <=> Y = sX + m, X ~ Normal(0,1)`), we can swap gradient and
expectation, i.e.,
`grad[Avg{ s_i : i=1...n }] = Avg{ grad[s_i] : i=1...n }` where `S_n=Avg{s_i}`
and `s_i = f(x_i), x_i ~iid q(X)`.

However, if q is not reparameterized, TensorFlow's gradient will be incorrect
since the chain-rule stops at samples of unreparameterized distributions. In
this circumstance using the Score-Gradient trick results in an unbiased
gradient, i.e.,

```none
grad[ E_q[f(X)] ]
= grad[ int dx q(x) f(x) ]
= int dx grad[ q(x) f(x) ]
= int dx [ q'(x) f(x) + q(x) f'(x) ]
= int dx q(x) [q'(x) / q(x) f(x) + f'(x) ]
= int dx q(x) grad[ f(x) q(x) / stop_grad[q(x)] ]
= E_q[ grad[ f(x) q(x) / stop_grad[q(x)] ] ]
```

Unless `q.reparameterization_type != distribution.FULLY_REPARAMETERIZED` it is
usually preferable to set `use_reparametrization = True`.

Example Application:

The Csiszar f-Divergence is a useful framework for variational inference.
I.e., observe that,

```none
f(p(x)) =  f( E_{q(Z | x)}[ p(x, Z) / q(Z | x) ] )
        <= E_{q(Z | x)}[ f( p(x, Z) / q(Z | x) ) ]
        := D_f[p(x, Z), q(Z | x)]
```

The inequality follows from the fact that the "perspective" of `f`, i.e.,
`(s, t) |-> t f(s / t))`, is convex in `(s, t)` when `s/t in domain(f)` and
`t` is a real. Since the above framework includes the popular Evidence Lower
BOund (ELBO) as a special case, i.e., `f(u) = -log(u)`, we call this framework
"Evidence Divergence Bound Optimization" (EDBO).

#### Args:

* <b>`f`</b>: Python callable representing a Csiszar-function in log-space.
* <b>`p`</b>: `tf.Distribution`-like instance; must implement `log_prob(x)`.
* <b>`q`</b>: `tf.Distribution`-like instance; must implement:
    `reparameterization_type`, `sample(n)`, and `log_prob(x)`.
* <b>`num_draws`</b>: Integer scalar number of draws used to approximate the
    f-Divergence expectation.
* <b>`use_reparametrization`</b>: Python `bool`. When `True` uses the standard
    Monte-Carlo average. When `False` uses the score-gradient trick. (See
    above for details.)
* <b>`seed`</b>: Python `int` seed for `q.sample`.
* <b>`name`</b>: Python `str` name prefixed to Ops created by this function.


#### Returns:

* <b>`monte_carlo_csiszar_f_divergence`</b>: Floating-type `Tensor` Monte Carlo
    approximation of the Csiszar f-Divergence.


#### Raises:

* <b>`ValueError`</b>: if `q` is not a reparameterized distribution and
    `use_reparametrization = True`. A distribution `q` is said to be
    "reparameterized" when its samples are generated by transforming the
    samples of another distribution which does not depend on the
    parameterization of `q`. This property ensures the gradient (with respect
    to parameters) is valid.