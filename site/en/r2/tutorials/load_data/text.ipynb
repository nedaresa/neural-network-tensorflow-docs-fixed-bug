{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DweYe9FcbMK_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AVV2e0XKbJeX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sUtoed20cRJJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load text with tf.data"
      ]
    },
    {
      "metadata": {
        "id": "1ap_W4aQcgNT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/alpha/tutorials/load_data/text\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "NWeQAo0Ec_BL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial provides an example of how to use `tf.data.Dataset` to load examples from text files. It will also cover preprocessing and preparing text to be usable in a model."
      ]
    },
    {
      "metadata": {
        "id": "FSjRvs9r5eHH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*mention keras preprocessing and some tradeoffs*"
      ]
    },
    {
      "metadata": {
        "id": "fgZ9gjmPfSnK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ]
    },
    {
      "metadata": {
        "id": "ahCrZgPtYv5H",
        "colab_type": "code",
        "outputId": "660530a8-8049-491e-994e-e29bd32e76e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/84/970bfb6eb04360a675627a38962127f0f5302ac1fd3ac4ad4f5d1befc9b7/tensorflow-2.0.0a0-cp27-cp27mu-manylinux1_x86_64.whl (79.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 79.9MB 235kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.post1)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/4f/369f43db86ee663826dc4a7cce7e18b3f9c58c8defc9e78368230b015d2b/tb_nightly-1.14.0a20190301-py2-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 7.4MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.2 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/7f/0471cd7d94df22a09e92e06b5722b58c9fca71c6617347fdbf4b88206a0d/google_pasta-0.1.4-py2-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 19.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K    100% |████████████████████████████████| 419kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==2.0.0-alpha0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (5.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.15.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.1)\n",
            "Installing collected packages: tb-nightly, google-pasta, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.13.1\n",
            "    Uninstalling tensorflow-1.13.1:\n",
            "      Successfully uninstalled tensorflow-1.13.1\n",
            "Successfully installed google-pasta-0.1.4 tb-nightly-1.14.0a20190301 tensorflow-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "baYFZMW_bJHh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function \n",
        "\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWVWjyIkffau",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial shows you how to load text into a TensorFlow dataset. So, before we begin, we're going to download and extract the text examples into a local directory.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hRI0ZlhTgQ8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DIRECTORY_URL = 'https://s3.amazonaws.com/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "for name in FILE_NAMES:\n",
        "  r = requests.get(\"\".join([DIRECTORY_URL, name]), allow_redirects=True)\n",
        "  with open(name, 'w') as f:\n",
        "    f.write(r.content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "54Dv7mCrf9Yw",
        "colab_type": "code",
        "outputId": "6ef91a0b-609e-4997-fa02-af9a0b9f1b25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "butler.txt  cowper.txt\tderby.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K0BjCOpOh7Ch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_sets = []\n",
        "labeled_data_sets = []\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "  lines_dataset = tf.data.TextLineDataset(file_name)\n",
        "  labeled_dataset = lines_dataset.map(lambda x: (x, tf.cast(i, tf.int64)))\n",
        "  data_sets.append(lines_dataset)\n",
        "  labeled_data_sets.append(labeled_dataset)\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qd544E-Sh63L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_labeled_data = labeled_data_sets[0].concatenate(labeled_data_sets[1]).concatenate(labeled_data_sets[2])\n",
        "# interleave?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4PNww-H2h6xw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_labeled_data = all_labeled_data.shuffle(tf.cast(50000, tf.int64), reshuffle_each_iteration=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gywKlN0xh6u5",
        "colab_type": "code",
        "outputId": "d7d2128f-3ec6-4a6e-b9d8-53737cd236c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "cell_type": "code",
      "source": [
        "for ex in all_labeled_data.take(5):\n",
        "  print(ex)\n",
        "  \n",
        "for ex in all_labeled_data.take(5):\n",
        "  print(ex)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: id=2610384, shape=(), dtype=string, numpy='We Gods endure, we each to other owe'>, <tf.Tensor: id=2610385, shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: id=2610388, shape=(), dtype=string, numpy='But him his sister, Goddess of the chase,'>, <tf.Tensor: id=2610389, shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: id=2610392, shape=(), dtype=string, numpy='The ruthless point, and, falling, he expired.'>, <tf.Tensor: id=2610393, shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: id=2610396, shape=(), dtype=string, numpy=\"His comrades mourning; with them, Peleus' son,\">, <tf.Tensor: id=2610397, shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: id=2610400, shape=(), dtype=string, numpy='\"Haste to Achilles\\' tent, and in your hand'>, <tf.Tensor: id=2610401, shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: id=2610410, shape=(), dtype=string, numpy='We Gods endure, we each to other owe'>, <tf.Tensor: id=2610411, shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: id=2610414, shape=(), dtype=string, numpy='But him his sister, Goddess of the chase,'>, <tf.Tensor: id=2610415, shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: id=2610418, shape=(), dtype=string, numpy='The ruthless point, and, falling, he expired.'>, <tf.Tensor: id=2610419, shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: id=2610422, shape=(), dtype=string, numpy=\"His comrades mourning; with them, Peleus' son,\">, <tf.Tensor: id=2610423, shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: id=2610426, shape=(), dtype=string, numpy='\"Haste to Achilles\\' tent, and in your hand'>, <tf.Tensor: id=2610427, shape=(), dtype=int64, numpy=1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vQImbgs8h6r5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YkHtbGnDh6mg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# build vocabulary\n",
        "\n",
        "tokenizer = tfds.features.text.Tokenizer()\n",
        "vocabulary_set = set()\n",
        "for text_tensor, _ in all_labeled_data:\n",
        "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  vocabulary_set.update(some_tokens)\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IFIxydXc7N_v",
        "colab_type": "code",
        "outputId": "5457c49b-24f1-4aa7-ad9f-54a18d08e28e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocabulary_set)\n",
        "vocab_size"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17178"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "metadata": {
        "id": "gkxJIVAth6j0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jgxPZaxUuTbk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = encoder.encode(next(iter(all_labeled_data))[0].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DapSJus3h6g_",
        "colab_type": "code",
        "outputId": "958e8977-767e-43cb-eb5b-7244a380299e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "encoder.decode(x)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u'We Gods endure we each to other owe'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "metadata": {
        "id": "t1HvttOWud5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b26a6726-f3c2-42bc-84b0-6533684c6382"
      },
      "cell_type": "code",
      "source": [
        "x"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10229, 2368, 464, 4935, 13208, 2202, 17136, 17048]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "metadata": {
        "id": "HcIQ7LOTh6eT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encode(text_tensor, label):\n",
        "  encoded_text = encoder.encode(text_tensor.numpy())\n",
        "  encoded_padded_text = tf.pad(encoded_text, [[0, MAX_LINE_LEN - len(encoded_text)]], 'CONSTANT')\n",
        "  return encoded_text, label\n",
        "  \n",
        "all_encoded_data = all_labeled_data.map(lambda text, label: tf.py_function(\n",
        "    encode, inp=[text, label], Tout=(tf.int64, tf.int64)\n",
        "))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSFgMNsnaBqS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MAX_LINE_LEN = max([tf.size(ex[0]) for ex in all_encoded_data]).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qMGoD2EubPJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ee8b50db-5e2d-4ac9-f30f-22a1b57fdc50"
      },
      "cell_type": "code",
      "source": [
        "MAX_LINE_LEN"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "metadata": {
        "id": "Xd8bDEpx7iCJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "018ea080-b948-45d0-dd26-d237c60abea8"
      },
      "cell_type": "code",
      "source": [
        "all_encoded_data._element_structure._flat_shapes_list"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TensorShape(None), TensorShape(None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "metadata": {
        "id": "r-rmbijQh6bf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test_data = all_encoded_data.take(5000)\n",
        "# train_data = all_encoded_data.skip(5000).padded_batch(64, padded_shapes=all_encoded_data._element_structure._flat_shapes)\n",
        "\n",
        "test_data = all_encoded_data.take(5000)\n",
        "train_data = all_encoded_data.skip(5000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pkTBUVO4h6Y5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_model(input_dim, embedding_dim=50, hidden_units=[100]):\n",
        "  \"\"\"Create a Keras Sequential model with layers.\n",
        "\n",
        "  Args:\n",
        "    input_dim: (int) Input dimensions for input layer.\n",
        "    embedding_dim: (int) Embedding dimension for embedding layer.\n",
        "    hidden_units: [int] the layer sizes of the DNN (input layer first)\n",
        "\n",
        "  Returns:\n",
        "    A Keras model.\n",
        "  \"\"\"\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(input_dim+1, 50))\n",
        "  model.add(tf.keras.layers.Conv1D(10, 2, padding='same'))\n",
        "  model.add(tf.keras.layers.Conv1D(10, 2, padding='same'))\n",
        "  model.add(tf.keras.layers.Conv1D(10, 2, padding='same'))\n",
        "  #model.add(tf.keras.layers.GlobalMaxPool1D())\n",
        "  for units in hidden_units:\n",
        "    model.add(tf.keras.layers.Dense(units, activation=tf.keras.backend.relu))\n",
        "  model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zhWfDlyp6UOK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aLtO33tNh6V8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "78b692e4-72a6-4733-f4e6-67ef96f4d99f"
      },
      "cell_type": "code",
      "source": [
        "model = get_model(vocab_size)\n",
        "model.fit(train_data, epochs=3)\n",
        "\n",
        "\n",
        "model.evaluate(test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "   4912/Unknown - 103s 21ms/step - loss: 0.9942 - accuracy: 0.3256"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KTPCYf_Jh6TH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pr05omK9h6Qd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YmCIA-vdh6N1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u3G3I-z6h6K7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A4UFth9mh6IV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yunx9cTJh6Fv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aCMTI-rRh6C7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oDYpm_B1h6AG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ih0CBbN3hv_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make text files available to Python\n",
        "\n",
        "In the local directory, examples are split into `train` and `test` directories. Within each of those, positive reviews will be in a directory called `pos`, and negative ones in a directory called `neg`.\n",
        "\n",
        "Your text data is probably organized differently than this, and may be in a database or other format. The important thing to notice in this step is making the text files available in a Python iterable. In this example, the iterable is a list of file names."
      ]
    },
    {
      "metadata": {
        "id": "v_Hts93IiW-P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_files = [\n",
        "    os.path.join(path, 'train', label, '*') for label in ['pos', 'neg']\n",
        "]\n",
        "test_files = [\n",
        "    os.path.join(path, 'test', label, '*') for label in ['pos', 'neg']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wWnPL8gXpoJg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create TensorFlow datasets\n",
        "\n",
        "We need to turn a bunch of files into labeled data. \n",
        "\n",
        "The original dataTo do this:\n",
        "\n",
        "1. Use `tf.data.Dataset.list_files` to create a Dataset of file names.\n",
        "2. Use `tf.data.Dataset.flat_map` to iterate through each file name and:\n",
        "\n",
        "  a. Label the item `1` for positive or `0` for negative.\n",
        "  \n",
        "  b. Load the text from the file with `tf.data.TextLineDataset`.\n",
        "  \n",
        "  c. Combine the label with the text data using `tf.data.Dataset.zip`.\n",
        "\n",
        "Apply this process to both the training data files and the test data files.\n"
      ]
    },
    {
      "metadata": {
        "id": "tNvmhqWep66H",
        "colab_type": "code",
        "outputId": "57899755-2196-416c-ef34-950b8ccfcb39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        }
      },
      "cell_type": "code",
      "source": [
        "def get_labeled_dataset(patterns):\n",
        "\n",
        "  files = tf.data.Dataset.list_files(patterns).shuffle(len(patterns))\n",
        "  \n",
        "  # Maps a filename to a dataset that produces (review, sentiment) pair.\n",
        "  def flat_map_fn(filename):\n",
        "    label = tf.data.Dataset.from_tensors(\n",
        "        tf.cast(tf.strings.regex_full_match(filename, '^.*pos.*$'), tf.float64))\n",
        "    return tf.data.Dataset.zip((tf.data.TextLineDataset(filename), label))\n",
        "\n",
        "  \n",
        "  return files.flat_map(flat_map_fn)\n",
        "\n",
        "train_data = get_labeled_dataset(train_files)\n",
        "test_data = get_labeled_dataset(test_files)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-c3df1ad11c1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_map_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labeled_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labeled_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-91-c3df1ad11c1a>\u001b[0m in \u001b[0;36mget_labeled_dataset\u001b[0;34m(patterns)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_labeled_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Maps a filename to a dataset that produces (review, sentiment) pair.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/data/ops/dataset_ops.pyc\u001b[0m in \u001b[0;36mlist_files\u001b[0;34m(file_pattern, shuffle, seed)\u001b[0m\n\u001b[1;32m    665\u001b[0m       file_pattern = ops.convert_to_tensor(\n\u001b[1;32m    666\u001b[0m           file_pattern, dtype=dtypes.string, name=\"file_pattern\")\n\u001b[0;32m--> 667\u001b[0;31m       \u001b[0mmatching_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_io_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_pattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m       \u001b[0;31m# Raise an exception if `file_pattern` does not match any files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_io_ops.pyc\u001b[0m in \u001b[0;36mmatching_files\u001b[0;34m(pattern, name)\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/six.pyc\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: /root/.keras/datasets/illiad/train/pos; No such file or directory [Op:MatchingFiles]"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "nDswcAqo0Ad4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.features.text.Tokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zz7tkaHA0AbB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ex = next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7c-ea6WI0AX5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized_ex = tokenizer.tokenize(ex[0].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mg8bXSUE0ASP",
        "colab_type": "code",
        "outputId": "c8777496-0803-460f-f7dd-f1df9da0c323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "# create vocabulary\n",
        "vocabulary_list = []\n",
        "for text_tensor, label_tensor in train_data.concatenate(test_data):\n",
        "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  vocabulary_list = vocabulary_list + some_tokens\n",
        "  \n",
        "len(vocabulary_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3e43568f05ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0msome_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mvocabulary_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msome_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3S1fXu9v0APb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = next(iter(train_files))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GsL7vEaqvLlr",
        "colab_type": "code",
        "outputId": "7629e72d-770b-4398-bea2-1616291b6f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "filename"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets/aclImdb/train/pos/*'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "E1ap6tDG0AMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tl_ds = tf.data.TextLineDataset('/root/.keras/datasets/aclImdb/train/pos/0_9.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUdUoxlf0ABu",
        "colab_type": "code",
        "outputId": "81d5fc6d-635c-47ca-e596-1e7a0dca68f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "for text_line in tl_ds.take(5):\n",
        "  print(text_line.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sLLp0vIEz_6V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5EKG4cN2z_xt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GMJxfbyhidID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build a vocabulary\n",
        "\n",
        "There are many ways to represent text data for input into a machine learning model. For this tutorial, the text of each review will be represented as a list of integers, with each integer representing a single unique word.\n",
        "\n",
        "The first step to doing this is to create a map of unique words to integers. This mapping will be called the `vocabulary`, and will be a Python dictionary with words as keys and integer Tensors as values.\n",
        "\n",
        "To build the vocabulary:\n",
        "\n",
        "1. Use `tf.data.Dataset.map` to tokenize each example:\n",
        "\n",
        "  a. Remove punctuation and other non-word characters from the text examples.\n",
        "  \n",
        "  b. Split them into arrays of tokens (word-like substrings).\n",
        "\n",
        "2. Create a new dataset in which each element is a token from the text data.\n",
        "\n",
        "3. Remove duplicates eith `tf.data.experimental.unique`.\n",
        "\n",
        "4. Assign a unique integer to each token with `tf.data.experimental.Counter`.\n",
        "\n",
        "5. Create a Python dictionary in which the keys are tokens and the values are integer tensors.\n",
        "\n",
        "6. Pass the keys (words) and values (integers) to `tf.lookup.KeyValueTensorInitializer`.\n",
        "\n",
        "7. Use the initializer to create a `tf.lookup.StaticVocabularyTable`.\n",
        "\n",
        "The `StaticVocabularyTable` holds the mapping of word tokens to integers, and also handles the encoding of word lists to integer lists."
      ]
    },
    {
      "metadata": {
        "id": "3XWR8cdtzxQi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  # Replace line breaks with spaces.\n",
        "  text = tf.strings.regex_replace(text, r'\\<br \\/\\>', ' ')\n",
        "  # Replace punctuation with spaces.\n",
        "  text = tf.strings.regex_replace(text, r'\\W', ' ')\n",
        "  # Turn the single long string into a list of strings.\n",
        "  tokens = tf.strings.split([text], sep=\" \").values\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def get_vocabulary_table(dataset):\n",
        "  # Tokenize the example text and drop the label.\n",
        "  dataset = dataset.map(lambda text, label: tokenize(text))\n",
        "  # Gather all the word tensors into a single dataset.\n",
        "  dataset = dataset.flat_map(tf.data.Dataset.from_tensor_slices)\n",
        "  # Remove duplicates.\n",
        "  dataset = dataset.apply(tf.data.experimental.unique())\n",
        "  # Assign an integer to each token.\n",
        "  dataset = tf.data.Dataset.zip((dataset, tf.data.experimental.Counter()))\n",
        "  # Turn (word, integer) pairs into a dict, so they can be passed easily into initializer.\n",
        "  vocabulary_dict = {word.numpy():index for word, index in iter(dataset)}\n",
        "  \n",
        "  vocabulary_table_initializer = tf.lookup.KeyValueTensorInitializer(\n",
        "      vocabulary_dict.keys(), \n",
        "      vocabulary_dict.values(), \n",
        "      tf.string\n",
        "  )\n",
        "  \n",
        "  return tf.lookup.StaticVocabularyTable(vocabulary_table_initializer, 1)\n",
        "  \n",
        "vocabulary_table = get_vocabulary_table(train_data.concatenate(test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q56pyygvo3qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `tf.lookup.StaticVocabularyTable` has a method `lookup`, which converts a list of words into a list of integers.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "metadata": {
        "id": "a3fibNrdbwvw",
        "colab_type": "code",
        "outputId": "ee6bfb27-aacf-4766-dd76-d3d788cce2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "vocabulary_table.lookup(tf.constant(['I', 'loved', 'this', 'movie']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=1427749, shape=(4,), dtype=int64, numpy=array([  1, 516,  86,  69])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "mQdocfTF4K7Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Turn text datasets into integer datasets\n",
        "\n",
        "Now that we have a numbered vocabulary, we can encode each review as a Tensor of integers.\n",
        "\n",
        "Each input Tensor needs to be the same length. So, first determine the length of the longest review. Then, tokenize the examples and encode them as a lists of integers, with a padding of zeroes to make all examples the same length.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "j9dCWyGDrzka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TDlJPejqsaJP",
        "colab_type": "code",
        "outputId": "61fe6845-f075-45fc-a22d-b9008eccd0b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: id=1427774, shape=(), dtype=string, numpy='I mean seriously what group would sing about a crazy car? So what if their ten, It\\'s way too immature for a little kid to sing about \"being my women\" I mean seriously! The name is pretty corny too, naked brothers? just because they take off their pants??? HOW CREATIVE.I don\\'t get why they need a TV show I mean most artist don\\'t really need a TV show about themselves, especially the naked brothers band. Heck how many of them are in the freaking group. And seriously whats with the movie? Jeez Nick use to be the hightlight of my years growing up but seriously The naked brother band? SO many parents would not let their kids watch this especially with the name the Naked Brother\\'s band, its a stupid, uncreative show that should not be aired onto TV.'>,\n",
              " <tf.Tensor: id=1427775, shape=(), dtype=float64, numpy=0.0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "4CE8szR_rzZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized_train_data = train_data.map(lambda text, label: (tokenize(text), label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0NOibWNd4UtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_int_encoded_dataset(vocabulary, dataset):\n",
        "\n",
        "  def encode_and_pad(tokenized_text, label):\n",
        "\n",
        "    def helper(tokenized_text):\n",
        "      tokenized_text = tokenized_text.numpy()\n",
        "      result = []\n",
        "      for word in tokenized_text:\n",
        "        result.append(vocabulary[word])\n",
        "      return tf.pad(result, [[0, MAX_LEN - len(result)]], 'CONSTANT')\n",
        "\n",
        "    return tf.py_function(helper, [tokenized_text], tf.int64), label\n",
        "\n",
        "  dataset = dataset.map(tokenize)\n",
        "  dataset = dataset.map(encode_and_pad)\n",
        "  dataset = dataset.shuffle(10 * BATCH_SIZE)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  return dataset\n",
        "\n",
        "train_data = get_int_encoded_dataset(vocabulary, train_data)\n",
        "\n",
        "\n",
        "# THINGS TO TRY\n",
        "# ragged tensor\n",
        "# padded batch\n",
        "# bucket by sequence length\n",
        "# feature columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "67YbWWli8bwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_batch,label_batch = next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJwt5dRS87E8",
        "colab_type": "code",
        "outputId": "d5f19704-6087-4d67-f467-b85b7b30afd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "text_batch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=1753688, shape=(512, 2470), dtype=int64, numpy=\n",
              "array([[  321,    85,   510, ...,     0,     0,     0],\n",
              "       [  421,   325,  4682, ...,     0,     0,     0],\n",
              "       [    0,   187,    85, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [ 1481,    11, 10541, ...,     0,     0,     0],\n",
              "       [  242, 10129,     6, ...,     0,     0,     0],\n",
              "       [ 6043,   245,   372, ...,     0,     0,     0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "tALi4WYq9I86",
        "colab_type": "code",
        "outputId": "980c7e49-63cd-4124-e0e8-3cef12430716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.pcolormesh(text_batch.numpy() != 0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.QuadMesh at 0x7f0547253110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQv1SNaZiVrsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "FKIow6eYlnSn",
        "colab_type": "code",
        "outputId": "fecd1ff0-f8a5-4b46-dca2-2010f853f192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1095
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_model(input_dim, embedding_dim=50, hidden_units=[100]):\n",
        "  \"\"\"Create a Keras Sequential model with layers.\n",
        "\n",
        "  Args:\n",
        "    input_dim: (int) Input dimensions for input layer.\n",
        "    embedding_dim: (int) Embedding dimension for embedding layer.\n",
        "    hidden_units: [int] the layer sizes of the DNN (input layer first)\n",
        "\n",
        "  Returns:\n",
        "    A Keras model.\n",
        "  \"\"\"\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(input_dim=input_dim,\n",
        "                                      output_dim=embedding_dim,\n",
        "                                      input_length=MAX_LEN))\n",
        "  # convolutional layer or RNN\n",
        "  model.add(tf.keras.layers.GlobalMaxPool1D())\n",
        "  for units in hidden_units:\n",
        "    model.add(tf.keras.layers.Dense(units, activation=tf.keras.backend.relu))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = get_model(len(vocabulary))\n",
        "model.fit(train_data, epochs=10)\n",
        "\n",
        "test_data = get_indexed_dataset(vocabulary, test_files)\n",
        "model.evaluate(test_data)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-alpha0 in /usr/local/lib/python2.7/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.1.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.post1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==2.0.0-alpha0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (5.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.0.1)\n",
            "345418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0321 22:29:00.914769 140374431422336 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/script_ops.py:476: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 38s 769ms/step - loss: 0.6856 - accuracy: 0.5904\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 36s 744ms/step - loss: 0.5781 - accuracy: 0.8380\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 37s 749ms/step - loss: 0.3396 - accuracy: 0.8825\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 37s 750ms/step - loss: 0.1999 - accuracy: 0.9308\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 39s 800ms/step - loss: 0.1140 - accuracy: 0.9659\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 38s 771ms/step - loss: 0.0593 - accuracy: 0.9877\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 37s 765ms/step - loss: 0.0296 - accuracy: 0.9963\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 37s 757ms/step - loss: 0.0152 - accuracy: 0.9990\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 36s 743ms/step - loss: 0.0085 - accuracy: 0.9998\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 36s 742ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "     49/Unknown - 24s 496ms/step - loss: 0.3709 - accuracy: 0.8570"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37087987332927935, 0.85696]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    }
  ]
}