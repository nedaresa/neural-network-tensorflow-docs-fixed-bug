{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DweYe9FcbMK_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AVV2e0XKbJeX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sUtoed20cRJJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load text with tf.data"
      ]
    },
    {
      "metadata": {
        "id": "1ap_W4aQcgNT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/alpha/tutorials/load_data/text\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "NWeQAo0Ec_BL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial provides an example of how to use `tf.data.Dataset` to load examples from text files. It will also cover preprocessing and preparing text to be usable in a model. Finally, we'll do basic sentiment analysis, classifying each text example as \"positive\" or \"negative.\" The text examples are movie reviews from IMDB."
      ]
    },
    {
      "metadata": {
        "id": "fgZ9gjmPfSnK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ]
    },
    {
      "metadata": {
        "id": "baYFZMW_bJHh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function \n",
        "\n",
        "!pip install tensorflow==2.0.0-alpha0\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWVWjyIkffau",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial shows you how to load text into a TensorFlow dataset. So, before we begin, we're going to download and extract the text examples into a local directory.\n",
        "\n",
        "In the local directory, examples are split into `train` and `test` directories. Within each of those positive reviews will be in a directory called `pos`, and negative ones in a directory called `neg`."
      ]
    },
    {
      "metadata": {
        "id": "hRI0ZlhTgQ8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_URL = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        "file = 'aclImdb_v1.tar.gz'\n",
        "file = tf.keras.utils.get_file(file, DATA_URL, extract=True)\n",
        "path = os.path.join(os.path.dirname(file), 'aclImdb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ih0CBbN3hv_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make text files available to Python\n",
        "\n",
        "Your text data is probably organized differently than this, and may be in a database or other format. The important thing to notice in this step is making the text files available in a Python iterable."
      ]
    },
    {
      "metadata": {
        "id": "v_Hts93IiW-P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_files = [\n",
        "    os.path.join(path, 'train', label, '*') for label in ['pos', 'neg']\n",
        "]\n",
        "test_files = [\n",
        "    os.path.join(path, 'test', label, '*') for label in ['pos', 'neg']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GMJxfbyhidID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build a vocabulary\n",
        "\n",
        "There are many ways to represent text data for input into a machine learning model. We're going to use a representation called \"[bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model)\".\n",
        "\n",
        "Each movie review will be converted into a sparse vector. Each \"slot\" in the vector will represent a single word. If the word appears in the review, the slot has a `1`, otherwise the spot has a `0`. The length of the vector (the number of slots) will correspond to the number of individual uniqe words in the original set of text examples.\n",
        "\n",
        "The first step to doing this is to create a map of integers (repesenting slots in the sparse vectors) and unique words. This mapping will be called the `vocabulary`. To build the vocabulary:\n",
        "\n",
        "1. Remove punctuation and other non-word characters from the text examples.\n",
        "2. Tokenize the text examples (split them into arrays of words).\n",
        "3. Iterate through each array of words, adding unique words to the vocabulary index. "
      ]
    },
    {
      "metadata": {
        "id": "FKIow6eYlnSn",
        "colab_type": "code",
        "outputId": "fecd1ff0-f8a5-4b46-dca2-2010f853f192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1095
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "BATCH_SIZE = 512\n",
        "MAX_LEN = 256\n",
        "NUM_REVIEWS = 25000\n",
        "\n",
        "\n",
        "def get_model(input_dim, embedding_dim=50, hidden_units=[100]):\n",
        "  \"\"\"Create a Keras Sequential model with layers.\n",
        "\n",
        "  Args:\n",
        "    input_dim: (int) Input dimensions for input layer.\n",
        "    embedding_dim: (int) Embedding dimension for embedding layer.\n",
        "    hidden_units: [int] the layer sizes of the DNN (input layer first)\n",
        "\n",
        "  Returns:\n",
        "    A Keras model.\n",
        "  \"\"\"\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(input_dim=input_dim,\n",
        "                                      output_dim=embedding_dim,\n",
        "                                      input_length=MAX_LEN))\n",
        "  model.add(tf.keras.layers.GlobalMaxPool1D())\n",
        "  for units in hidden_units:\n",
        "    model.add(tf.keras.layers.Dense(units, activation=tf.keras.backend.relu))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "def get_labeled_dataset(patterns):\n",
        "\n",
        "  # Maps a filename to a dataset that produces (review, sentiment) pair.\n",
        "  def flat_map_fn(filename):\n",
        "    label = tf.data.Dataset.from_tensors(\n",
        "        tf.cast(tf.strings.regex_full_match(filename, '^.*pos.*$'), tf.float64))\n",
        "    return tf.data.Dataset.zip((tf.data.TextLineDataset(filename), label))\n",
        "\n",
        "  files = tf.data.Dataset.list_files(patterns).shuffle(NUM_REVIEWS)\n",
        "  return files.flat_map(flat_map_fn)\n",
        "\n",
        "\n",
        "def tokenize(line, label):\n",
        "  # Replace line breaks with spaces.\n",
        "  line = tf.strings.regex_replace(line, r'\\<br \\/\\>', ' ')\n",
        "  # Replace periods with spaces.\n",
        "  line = tf.strings.regex_replace(line, r'\\.', ' ')\n",
        "  tokens = tf.strings.split([line], sep=\" \").values\n",
        "  return tokens, label\n",
        "\n",
        "\n",
        "def get_vocabulary(patterns):\n",
        "  dataset = get_labeled_dataset(patterns)\n",
        "  dataset = dataset.map(tokenize)\n",
        "  dataset = dataset.flat_map(lambda x, y: tf.data.Dataset.from_tensor_slices(x))\n",
        "  dataset = dataset.apply(tf.data.experimental.unique())\n",
        "  dataset = tf.data.Dataset.zip((dataset, tf.data.experimental.Counter()))\n",
        "\n",
        "  vocabulary = {}\n",
        "  for word, index in iter(dataset):\n",
        "    vocabulary[word.numpy()] = index\n",
        "  return vocabulary\n",
        "\n",
        "\n",
        "def get_indexed_dataset(vocabulary, patterns):\n",
        "\n",
        "  def index_and_pad(word_list, label):\n",
        "\n",
        "    def helper(word_list):\n",
        "      result = []\n",
        "      for word, _ in zip(word_list, range(MAX_LEN)):\n",
        "        result.append(vocabulary[word])\n",
        "      return tf.pad(result, [[0, MAX_LEN - len(result)]], 'CONSTANT')\n",
        "\n",
        "    return tf.numpy_function(helper, [word_list], tf.int64), label\n",
        "\n",
        "  dataset = get_labeled_dataset(patterns)\n",
        "  dataset = dataset.map(tokenize)\n",
        "  dataset = dataset.map(index_and_pad)\n",
        "  dataset = dataset.shuffle(10 * BATCH_SIZE)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "vocabulary = get_vocabulary(train_files + test_files)\n",
        "print(len(vocabulary))\n",
        "train_data = get_indexed_dataset(vocabulary, train_files)\n",
        "\n",
        "model = get_model(len(vocabulary))\n",
        "model.fit(train_data, epochs=10)\n",
        "\n",
        "test_data = get_indexed_dataset(vocabulary, test_files)\n",
        "model.evaluate(test_data)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-alpha0 in /usr/local/lib/python2.7/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.1.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.post1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==2.0.0-alpha0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (5.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.0.1)\n",
            "345418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0321 22:29:00.914769 140374431422336 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/script_ops.py:476: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 38s 769ms/step - loss: 0.6856 - accuracy: 0.5904\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 36s 744ms/step - loss: 0.5781 - accuracy: 0.8380\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 37s 749ms/step - loss: 0.3396 - accuracy: 0.8825\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 37s 750ms/step - loss: 0.1999 - accuracy: 0.9308\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 39s 800ms/step - loss: 0.1140 - accuracy: 0.9659\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 38s 771ms/step - loss: 0.0593 - accuracy: 0.9877\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 37s 765ms/step - loss: 0.0296 - accuracy: 0.9963\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 37s 757ms/step - loss: 0.0152 - accuracy: 0.9990\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 36s 743ms/step - loss: 0.0085 - accuracy: 0.9998\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 36s 742ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "     49/Unknown - 24s 496ms/step - loss: 0.3709 - accuracy: 0.8570"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37087987332927935, 0.85696]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    }
  ]
}