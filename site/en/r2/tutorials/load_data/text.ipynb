{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DweYe9FcbMK_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AVV2e0XKbJeX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sUtoed20cRJJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load text with tf.data"
      ]
    },
    {
      "metadata": {
        "id": "1ap_W4aQcgNT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/alpha/tutorials/load_data/text\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "NWeQAo0Ec_BL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial provides an example of how to use `tf.data.Dataset` to load examples from text files. It will also cover preprocessing and preparing text to be usable in a model."
      ]
    },
    {
      "metadata": {
        "id": "FSjRvs9r5eHH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*mention keras preprocessing and some tradeoffs*"
      ]
    },
    {
      "metadata": {
        "id": "fgZ9gjmPfSnK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ]
    },
    {
      "metadata": {
        "id": "ahCrZgPtYv5H",
        "colab_type": "code",
        "outputId": "49448aae-699c-41e2-dfd8-090e774f88e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/84/970bfb6eb04360a675627a38962127f0f5302ac1fd3ac4ad4f5d1befc9b7/tensorflow-2.0.0a0-cp27-cp27mu-manylinux1_x86_64.whl (79.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 79.9MB 244kB/s \n",
            "\u001b[?25hRequirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.post1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/4f/369f43db86ee663826dc4a7cce7e18b3f9c58c8defc9e78368230b015d2b/tb_nightly-1.14.0a20190301-py2-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 8.0MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.2 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/7f/0471cd7d94df22a09e92e06b5722b58c9fca71c6617347fdbf4b88206a0d/google_pasta-0.1.4-py2-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 19.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K    100% |████████████████████████████████| 419kB 15.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==2.0.0-alpha0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (5.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.15.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.1)\n",
            "Installing collected packages: tb-nightly, google-pasta, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.13.1\n",
            "    Uninstalling tensorflow-1.13.1:\n",
            "      Successfully uninstalled tensorflow-1.13.1\n",
            "Successfully installed google-pasta-0.1.4 tb-nightly-1.14.0a20190301 tensorflow-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "baYFZMW_bJHh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function \n",
        "\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWVWjyIkffau",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial shows you how to load text into a TensorFlow dataset. So, before we begin, we're going to download and extract the text examples into a local directory.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hRI0ZlhTgQ8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DIRECTORY_URL = 'https://s3.amazonaws.com/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "#FILE_NAMES = ['french-text.txt', 'english-text.txt', 'german-text.txt']\n",
        "\n",
        "for name in FILE_NAMES:\n",
        "  r = requests.get(\"\".join([DIRECTORY_URL, name]), allow_redirects=True)\n",
        "  with open(name, 'w') as f:\n",
        "    f.write(r.content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "54Dv7mCrf9Yw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3b47231a-6ebf-4b00-b089-3b1d372fb617"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "butler.txt  cowper.txt\tderby.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "K0BjCOpOh7Ch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_sets = []\n",
        "labeled_data_sets = []\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "  lines_dataset = tf.data.TextLineDataset(file_name)\n",
        "  labeled_dataset = lines_dataset.map(lambda x: (x, tf.cast(i, tf.int64)))\n",
        "  data_sets.append(lines_dataset)\n",
        "  labeled_data_sets.append(labeled_dataset)\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qd544E-Sh63L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_labeled_data = labeled_data_sets[0].concatenate(labeled_data_sets[1]).concatenate(labeled_data_sets[2])\n",
        "# interleave?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4PNww-H2h6xw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_labeled_data = all_labeled_data.shuffle(tf.cast(50000, tf.int64), reshuffle_each_iteration=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gywKlN0xh6u5",
        "colab_type": "code",
        "outputId": "237787b4-a796-493f-db7c-3ddc93f98cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "for ex in all_labeled_data.take(5):\n",
        "  print(ex)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: id=86, shape=(), dtype=string, numpy='ranks holding his spear by the middle to keep them back, and they all'>, <tf.Tensor: id=87, shape=(), dtype=int64, numpy=2>)\n",
            "(<tf.Tensor: id=90, shape=(), dtype=string, numpy='Till he have roused Achilles, in that day'>, <tf.Tensor: id=91, shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: id=94, shape=(), dtype=string, numpy='Lift high the waters; mingle trees and stones'>, <tf.Tensor: id=95, shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: id=98, shape=(), dtype=string, numpy='With the long spear for Helen and the spoils'>, <tf.Tensor: id=99, shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: id=102, shape=(), dtype=string, numpy='Forth sprang the monarch first; he slew the Chief'>, <tf.Tensor: id=103, shape=(), dtype=int64, numpy=0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vQImbgs8h6r5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YkHtbGnDh6mg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# build vocabulary\n",
        "\n",
        "tokenizer = tfds.features.text.Tokenizer()\n",
        "vocabulary_set = set()\n",
        "for text_tensor, _ in all_labeled_data:\n",
        "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  vocabulary_set.update(some_tokens)\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IFIxydXc7N_v",
        "colab_type": "code",
        "outputId": "faa62369-1362-4eb4-f172-364e7f71b7ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocabulary_set)\n",
        "vocab_size"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17178"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "gkxJIVAth6j0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jgxPZaxUuTbk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = encoder.encode(next(iter(all_labeled_data))[0].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DapSJus3h6g_",
        "colab_type": "code",
        "outputId": "359dc265-d62a-48d3-b719-c668f8fdd81b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "encoder.decode(x)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "u'ranks holding his spear by the middle to keep them back and they all'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "t1HvttOWud5p",
        "colab_type": "code",
        "outputId": "4cf84b3c-e248-4efe-c01c-7fe80a8e3fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "cell_type": "code",
      "source": [
        "x"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[17169,\n",
              " 5301,\n",
              " 15612,\n",
              " 4944,\n",
              " 3094,\n",
              " 5077,\n",
              " 10390,\n",
              " 2198,\n",
              " 13077,\n",
              " 4561,\n",
              " 11617,\n",
              " 14422,\n",
              " 4573,\n",
              " 15034]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "HcIQ7LOTh6eT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encode(text_tensor, label):\n",
        "  encoded_text = encoder.encode(text_tensor.numpy())\n",
        "  encoded_padded_text = tf.pad(encoded_text, [[0, MAX_LINE_LEN - len(encoded_text)]], 'CONSTANT')\n",
        "  return encoded_text, label\n",
        "  \n",
        "all_encoded_data = all_labeled_data.map(lambda text, label: tf.py_function(\n",
        "    encode, inp=[text, label], Tout=(tf.int64, tf.int64)\n",
        "))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSFgMNsnaBqS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MAX_LINE_LEN = max([tf.size(ex[0]) for ex in all_encoded_data]).numpy()\n",
        "MAX_LINE_LEN = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qMGoD2EubPJr",
        "colab_type": "code",
        "outputId": "edb179ce-b6cf-4c2b-d656-7e277b7a2685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "MAX_LINE_LEN"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "r-rmbijQh6bf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_data = all_encoded_data.take(5000).padded_batch(50, padded_shapes=([-1],[]))\n",
        "train_data = all_encoded_data.skip(5000).padded_batch(50, padded_shapes=([-1],[]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pkTBUVO4h6Y5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_dimension = vocab_size + 1\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dimension, 64))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "for units in [64, 64]:\n",
        "  model.add(tf.keras.layers.Dense(units, activation=tf.keras.backend.relu))\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aLtO33tNh6V8",
        "colab_type": "code",
        "outputId": "c6ffdece-2e6f-472e-9fba-c9a0643f9b6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(train_data, epochs=3)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "893/893 [==============================] - 66s 73ms/step - loss: 0.5049 - accuracy: 0.7532\n",
            "Epoch 2/3\n",
            "893/893 [==============================] - 63s 71ms/step - loss: 0.2937 - accuracy: 0.8686\n",
            "Epoch 3/3\n",
            "893/893 [==============================] - 63s 71ms/step - loss: 0.2182 - accuracy: 0.9053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdea683ef90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "KTPCYf_Jh6TH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "91540b33-7406-4f8e-83e9-96d3a1e35d10"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(test_data)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    100/Unknown - 6s 57ms/step - loss: 0.4152 - accuracy: 0.8298"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4152007557451725, 0.8298]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "pr05omK9h6Qd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YmCIA-vdh6N1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u3G3I-z6h6K7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A4UFth9mh6IV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yunx9cTJh6Fv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aCMTI-rRh6C7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oDYpm_B1h6AG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ih0CBbN3hv_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make text files available to Python\n",
        "\n",
        "In the local directory, examples are split into `train` and `test` directories. Within each of those, positive reviews will be in a directory called `pos`, and negative ones in a directory called `neg`.\n",
        "\n",
        "Your text data is probably organized differently than this, and may be in a database or other format. The important thing to notice in this step is making the text files available in a Python iterable. In this example, the iterable is a list of file names."
      ]
    },
    {
      "metadata": {
        "id": "v_Hts93IiW-P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_files = [\n",
        "    os.path.join(path, 'train', label, '*') for label in ['pos', 'neg']\n",
        "]\n",
        "test_files = [\n",
        "    os.path.join(path, 'test', label, '*') for label in ['pos', 'neg']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wWnPL8gXpoJg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create TensorFlow datasets\n",
        "\n",
        "We need to turn a bunch of files into labeled data. \n",
        "\n",
        "The original dataTo do this:\n",
        "\n",
        "1. Use `tf.data.Dataset.list_files` to create a Dataset of file names.\n",
        "2. Use `tf.data.Dataset.flat_map` to iterate through each file name and:\n",
        "\n",
        "  a. Label the item `1` for positive or `0` for negative.\n",
        "  \n",
        "  b. Load the text from the file with `tf.data.TextLineDataset`.\n",
        "  \n",
        "  c. Combine the label with the text data using `tf.data.Dataset.zip`.\n",
        "\n",
        "Apply this process to both the training data files and the test data files.\n"
      ]
    },
    {
      "metadata": {
        "id": "tNvmhqWep66H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_labeled_dataset(patterns):\n",
        "\n",
        "  files = tf.data.Dataset.list_files(patterns).shuffle(len(patterns))\n",
        "  \n",
        "  # Maps a filename to a dataset that produces (review, sentiment) pair.\n",
        "  def flat_map_fn(filename):\n",
        "    label = tf.data.Dataset.from_tensors(\n",
        "        tf.cast(tf.strings.regex_full_match(filename, '^.*pos.*$'), tf.float64))\n",
        "    return tf.data.Dataset.zip((tf.data.TextLineDataset(filename), label))\n",
        "\n",
        "  \n",
        "  return files.flat_map(flat_map_fn)\n",
        "\n",
        "train_data = get_labeled_dataset(train_files)\n",
        "test_data = get_labeled_dataset(test_files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDswcAqo0Ad4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.features.text.Tokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zz7tkaHA0AbB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ex = next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7c-ea6WI0AX5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized_ex = tokenizer.tokenize(ex[0].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mg8bXSUE0ASP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create vocabulary\n",
        "vocabulary_list = []\n",
        "for text_tensor, label_tensor in train_data.concatenate(test_data):\n",
        "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  vocabulary_list = vocabulary_list + some_tokens\n",
        "  \n",
        "len(vocabulary_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3S1fXu9v0APb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = next(iter(train_files))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GsL7vEaqvLlr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E1ap6tDG0AMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tl_ds = tf.data.TextLineDataset('/root/.keras/datasets/aclImdb/train/pos/0_9.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUdUoxlf0ABu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for text_line in tl_ds.take(5):\n",
        "  print(text_line.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sLLp0vIEz_6V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5EKG4cN2z_xt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GMJxfbyhidID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build a vocabulary\n",
        "\n",
        "There are many ways to represent text data for input into a machine learning model. For this tutorial, the text of each review will be represented as a list of integers, with each integer representing a single unique word.\n",
        "\n",
        "The first step to doing this is to create a map of unique words to integers. This mapping will be called the `vocabulary`, and will be a Python dictionary with words as keys and integer Tensors as values.\n",
        "\n",
        "To build the vocabulary:\n",
        "\n",
        "1. Use `tf.data.Dataset.map` to tokenize each example:\n",
        "\n",
        "  a. Remove punctuation and other non-word characters from the text examples.\n",
        "  \n",
        "  b. Split them into arrays of tokens (word-like substrings).\n",
        "\n",
        "2. Create a new dataset in which each element is a token from the text data.\n",
        "\n",
        "3. Remove duplicates eith `tf.data.experimental.unique`.\n",
        "\n",
        "4. Assign a unique integer to each token with `tf.data.experimental.Counter`.\n",
        "\n",
        "5. Create a Python dictionary in which the keys are tokens and the values are integer tensors.\n",
        "\n",
        "6. Pass the keys (words) and values (integers) to `tf.lookup.KeyValueTensorInitializer`.\n",
        "\n",
        "7. Use the initializer to create a `tf.lookup.StaticVocabularyTable`.\n",
        "\n",
        "The `StaticVocabularyTable` holds the mapping of word tokens to integers, and also handles the encoding of word lists to integer lists."
      ]
    },
    {
      "metadata": {
        "id": "3XWR8cdtzxQi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  # Replace line breaks with spaces.\n",
        "  text = tf.strings.regex_replace(text, r'\\<br \\/\\>', ' ')\n",
        "  # Replace punctuation with spaces.\n",
        "  text = tf.strings.regex_replace(text, r'\\W', ' ')\n",
        "  # Turn the single long string into a list of strings.\n",
        "  tokens = tf.strings.split([text], sep=\" \").values\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def get_vocabulary_table(dataset):\n",
        "  # Tokenize the example text and drop the label.\n",
        "  dataset = dataset.map(lambda text, label: tokenize(text))\n",
        "  # Gather all the word tensors into a single dataset.\n",
        "  dataset = dataset.flat_map(tf.data.Dataset.from_tensor_slices)\n",
        "  # Remove duplicates.\n",
        "  dataset = dataset.apply(tf.data.experimental.unique())\n",
        "  # Assign an integer to each token.\n",
        "  dataset = tf.data.Dataset.zip((dataset, tf.data.experimental.Counter()))\n",
        "  # Turn (word, integer) pairs into a dict, so they can be passed easily into initializer.\n",
        "  vocabulary_dict = {word.numpy():index for word, index in iter(dataset)}\n",
        "  \n",
        "  vocabulary_table_initializer = tf.lookup.KeyValueTensorInitializer(\n",
        "      vocabulary_dict.keys(), \n",
        "      vocabulary_dict.values(), \n",
        "      tf.string\n",
        "  )\n",
        "  \n",
        "  return tf.lookup.StaticVocabularyTable(vocabulary_table_initializer, 1)\n",
        "  \n",
        "vocabulary_table = get_vocabulary_table(train_data.concatenate(test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q56pyygvo3qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `tf.lookup.StaticVocabularyTable` has a method `lookup`, which converts a list of words into a list of integers.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "metadata": {
        "id": "a3fibNrdbwvw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocabulary_table.lookup(tf.constant(['I', 'loved', 'this', 'movie']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mQdocfTF4K7Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Turn text datasets into integer datasets\n",
        "\n",
        "Now that we have a numbered vocabulary, we can encode each review as a Tensor of integers.\n",
        "\n",
        "Each input Tensor needs to be the same length. So, first determine the length of the longest review. Then, tokenize the examples and encode them as a lists of integers, with a padding of zeroes to make all examples the same length.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "j9dCWyGDrzka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TDlJPejqsaJP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4CE8szR_rzZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized_train_data = train_data.map(lambda text, label: (tokenize(text), label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0NOibWNd4UtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_int_encoded_dataset(vocabulary, dataset):\n",
        "\n",
        "  def encode_and_pad(tokenized_text, label):\n",
        "\n",
        "    def helper(tokenized_text):\n",
        "      tokenized_text = tokenized_text.numpy()\n",
        "      result = []\n",
        "      for word in tokenized_text:\n",
        "        result.append(vocabulary[word])\n",
        "      return tf.pad(result, [[0, MAX_LEN - len(result)]], 'CONSTANT')\n",
        "\n",
        "    return tf.py_function(helper, [tokenized_text], tf.int64), label\n",
        "\n",
        "  dataset = dataset.map(tokenize)\n",
        "  dataset = dataset.map(encode_and_pad)\n",
        "  dataset = dataset.shuffle(10 * BATCH_SIZE)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  return dataset\n",
        "\n",
        "train_data = get_int_encoded_dataset(vocabulary, train_data)\n",
        "\n",
        "\n",
        "# THINGS TO TRY\n",
        "# ragged tensor\n",
        "# padded batch\n",
        "# bucket by sequence length\n",
        "# feature columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "67YbWWli8bwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_batch,label_batch = next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJwt5dRS87E8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tALi4WYq9I86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.pcolormesh(text_batch.numpy() != 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKIow6eYlnSn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_model(input_dim, embedding_dim=50, hidden_units=[100]):\n",
        "  \"\"\"Create a Keras Sequential model with layers.\n",
        "\n",
        "  Args:\n",
        "    input_dim: (int) Input dimensions for input layer.\n",
        "    embedding_dim: (int) Embedding dimension for embedding layer.\n",
        "    hidden_units: [int] the layer sizes of the DNN (input layer first)\n",
        "\n",
        "  Returns:\n",
        "    A Keras model.\n",
        "  \"\"\"\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(input_dim=input_dim,\n",
        "                                      output_dim=embedding_dim,\n",
        "                                      input_length=MAX_LEN))\n",
        "  # convolutional layer or RNN\n",
        "  model.add(tf.keras.layers.GlobalMaxPool1D())\n",
        "  for units in hidden_units:\n",
        "    model.add(tf.keras.layers.Dense(units, activation=tf.keras.backend.relu))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = get_model(len(vocabulary))\n",
        "model.fit(train_data, epochs=10)\n",
        "\n",
        "test_data = get_indexed_dataset(vocabulary, test_files)\n",
        "model.evaluate(test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}