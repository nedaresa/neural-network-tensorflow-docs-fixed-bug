{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DweYe9FcbMK_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AVV2e0XKbJeX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sUtoed20cRJJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load text with tf.data"
      ]
    },
    {
      "metadata": {
        "id": "1ap_W4aQcgNT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/alpha/tutorials/load_data/text\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "NWeQAo0Ec_BL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial provides an example of how to use `tf.data.Dataset` to load examples from text files. It will also cover preprocessing and preparing text to be usable in a model. Finally, we'll do basic sentiment analysis, classifying each text example as \"positive\" or \"negative.\" The text examples are movie reviews from IMDB."
      ]
    },
    {
      "metadata": {
        "id": "fgZ9gjmPfSnK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ]
    },
    {
      "metadata": {
        "id": "ahCrZgPtYv5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "f46d6c81-7496-4f8f-fec3-d8c5991f4604"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/84/970bfb6eb04360a675627a38962127f0f5302ac1fd3ac4ad4f5d1befc9b7/tensorflow-2.0.0a0-cp27-cp27mu-manylinux1_x86_64.whl (79.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 79.9MB 169kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/4f/369f43db86ee663826dc4a7cce7e18b3f9c58c8defc9e78368230b015d2b/tb_nightly-1.14.0a20190301-py2-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Collecting google-pasta>=0.1.2 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/7f/0471cd7d94df22a09e92e06b5722b58c9fca71c6617347fdbf4b88206a0d/google_pasta-0.1.4-py2-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 20.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.post1)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K    100% |████████████████████████████████| 419kB 16.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==2.0.0-alpha0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (5.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.0.1)\n",
            "Installing collected packages: tb-nightly, google-pasta, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.13.1\n",
            "    Uninstalling tensorflow-1.13.1:\n",
            "      Successfully uninstalled tensorflow-1.13.1\n",
            "Successfully installed google-pasta-0.1.4 tb-nightly-1.14.0a20190301 tensorflow-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "baYFZMW_bJHh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function \n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWVWjyIkffau",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial shows you how to load text into a TensorFlow dataset. So, before we begin, we're going to download and extract the text examples into a local directory.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hRI0ZlhTgQ8Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "fe1010ec-954c-4724-f7fd-5abd071e69b8"
      },
      "cell_type": "code",
      "source": [
        "DATA_URL = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        "file = 'aclImdb_v1.tar.gz'\n",
        "file = tf.keras.utils.get_file(file, DATA_URL, extract=True)\n",
        "path = os.path.join(os.path.dirname(file), 'aclImdb')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 2s 0us/step\n",
            "84140032/84125825 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ih0CBbN3hv_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make text files available to Python\n",
        "\n",
        "In the local directory, examples are split into `train` and `test` directories. Within each of those, positive reviews will be in a directory called `pos`, and negative ones in a directory called `neg`.\n",
        "\n",
        "Your text data is probably organized differently than this, and may be in a database or other format. The important thing to notice in this step is making the text files available in a Python iterable. In this example, the iterable is a list of file names."
      ]
    },
    {
      "metadata": {
        "id": "v_Hts93IiW-P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_files = [\n",
        "    os.path.join(path, 'train', label, '*') for label in ['pos', 'neg']\n",
        "]\n",
        "test_files = [\n",
        "    os.path.join(path, 'test', label, '*') for label in ['pos', 'neg']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wWnPL8gXpoJg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create TensorFlow datasets\n",
        "\n",
        "We need to turn a bunch of files into labeled data. \n",
        "\n",
        "The original dataTo do this:\n",
        "\n",
        "1. Use `tf.data.Dataset.list_files` to create a Dataset of file names.\n",
        "2. Use `tf.data.Dataset.flat_map` to iterate through each file name and:\n",
        "\n",
        "  a. Label the item `1` for positive or `0` for negative.\n",
        "  \n",
        "  b. Load the text from the file with `tf.data.TextLineDataset`.\n",
        "  \n",
        "  c. Combine the label with the text data using `tf.data.Dataset.zip`.\n",
        "\n",
        "Apply this process to both the training data files and the test data files.\n"
      ]
    },
    {
      "metadata": {
        "id": "tNvmhqWep66H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_labeled_dataset(patterns):\n",
        "\n",
        "  files = tf.data.Dataset.list_files(patterns).shuffle(len(patterns))\n",
        "  \n",
        "  # Maps a filename to a dataset that produces (review, sentiment) pair.\n",
        "  def flat_map_fn(filename):\n",
        "    label = tf.data.Dataset.from_tensors(\n",
        "        tf.cast(tf.strings.regex_full_match(filename, '^.*pos.*$'), tf.float64))\n",
        "    return tf.data.Dataset.zip((tf.data.TextLineDataset(filename), label))\n",
        "\n",
        "  \n",
        "  return files.flat_map(flat_map_fn)\n",
        "\n",
        "train_data = get_labeled_dataset(train_files)\n",
        "test_data = get_labeled_dataset(test_files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GMJxfbyhidID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build a vocabulary\n",
        "\n",
        "There are many ways to represent text data for input into a machine learning model. For this tutorial, the text of each review will be represented as a list of integers, with each integer representing a single unique word.\n",
        "\n",
        "The first step to doing this is to create a map of unique words to integers. This mapping will be called the `vocabulary`, and will be a Python dictionary with words as keys and integer Tensors as values.\n",
        "\n",
        "To build the vocabulary:\n",
        "\n",
        "1. Use `tf.data.Dataset.map` to tokenize each example:\n",
        "\n",
        "  a. Remove punctuation and other non-word characters from the text examples.\n",
        "  \n",
        "  b. Split them into arrays of tokens (word-like substrings).\n",
        "\n",
        "2. Create a new dataset in which each element is a token from the text data.\n",
        "\n",
        "3. Remove duplicates eith `tf.data.experimental.unique`.\n",
        "\n",
        "4. Assign a unique integer to each token with `tf.data.experimental.Counter`.\n",
        "\n",
        "5. Create a Python dictionary in which the keys are tokens and the values are integer tensors."
      ]
    },
    {
      "metadata": {
        "id": "3XWR8cdtzxQi",
        "colab_type": "code",
        "outputId": "39d4c5f8-f3fe-4468-951c-abbf95e7ff29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "def tokenize(line):\n",
        "  # Replace line breaks with spaces.\n",
        "  line = tf.strings.regex_replace(line, r'\\<br \\/\\>', ' ')\n",
        "  # Replace punctuation with spaces.\n",
        "  line = tf.strings.regex_replace(line, r'\\W', ' ')\n",
        "  tokens = tf.strings.split([line], sep=\" \").values\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def get_vocabulary(dataset):\n",
        "  dataset = dataset.map(lambda example, label: tokenize(example))\n",
        "  dataset = dataset.flat_map(tf.data.Dataset.from_tensor_slices)\n",
        "  dataset = dataset.apply(tf.data.experimental.unique())\n",
        "  dataset = tf.data.Dataset.zip((dataset, tf.data.experimental.Counter(start=1)))\n",
        "\n",
        "  vocabulary = {}\n",
        "  for word, index in iter(dataset):\n",
        "    vocabulary[word.numpy()] = index\n",
        "  return vocabulary\n",
        "\n",
        "vocabulary = get_vocabulary(train_data.concatenate(test_data))\n",
        "\n",
        "\n",
        "print(len(vocabulary)) # the number of unique tokens"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "129672\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eKQRYSagqHtE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# shorten get_vocabulary to not produce a dictionary?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xgQUQncnPow6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "530cf4fe-6178-4978-891c-ec8abe8559b4"
      },
      "cell_type": "code",
      "source": [
        "ds = test_data.map(tokenize).flat_map(tf.data.Dataset.from_tensor_slices.apply(tf.data.experimental.unique())\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-31-27bcebdee93d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ds = test_data.map(tokenize).flat_map(tf.data.Dataset.from_tensor_slices.apply(tf.data.experimental.unique())\u001b[0m\n\u001b[0m                                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Fqkd3lJoqCEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c753deb-754f-4def-cab5-a22255cedf50"
      },
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: (), types: tf.string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "qY0ZnZdCT0c3",
        "colab_type": "code",
        "outputId": "8bd9683a-7cbb-40de-8604-166dc15ccdd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "next(iter(ds))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=389332, shape=(), dtype=string, numpy='Rowan'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "0ZQ4fdMEP89I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "ds_init = tf.lookup.KeyValueTensorInitializer(list(vocabulary.keys()), list(vocabulary.values()), tf.string)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZSVZbeKdVH2H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ds_table = tf.lookup.StaticVocabularyTable(ds_init, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SzGzjr83VRrP",
        "colab_type": "code",
        "outputId": "55685e61-61d9-4c69-d12a-252edb67a69f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "ds_table.lookup(tf.constant(['I', 'hated', 'this', 'movie']))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=389346, shape=(4,), dtype=int64, numpy=array([   8, 4012,   73,   96])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "te_NelRnk84Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ex = next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fp983QSzlSPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "7f279dec-4348-433d-f3b2-d5c0838e64e0"
      },
      "cell_type": "code",
      "source": [
        "ex[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=464356, shape=(), dtype=string, numpy=\"That's right, you heard me. I am a huge fan of James Patterson. I own 10 of his books, and I have read the entire series about Lindsey Boxer. In my opinion, the screenwriter should be shot. <br /><br />What right did any film maker have to slaughter a terrific work of fiction and make it into a mockery of the mystery genre? If I ever thought that Harry Potter was butchered, then Michael O'Hara has proved me wrong. <br /><br />I can only pray that the next screenwriter who tackles this fabulous book will do it a great deal more justice. To Michael O'Hara and Russell Mulcahy: don't quit your day job.\">"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "wPWdY8MUlSEw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ex_tokenized = tokenize(ex[0].numpy(), _)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lFUow0P8ly54",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "a24a9bd4-8cdb-4d81-89fe-f1a4a5dd4507"
      },
      "cell_type": "code",
      "source": [
        "ds_table.lookup(ex_tokenized[0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=464385, shape=(130,), dtype=int64, numpy=\n",
              "array([ 1,  2,  3,  4,  5,  6,  7,  4,  8,  9, 10, 11, 12, 13, 14, 15,  4,\n",
              "        8, 16, 17, 13, 18, 19,  4, 20,  8, 21, 22, 23, 24, 25, 26, 27, 28,\n",
              "        4, 29, 30, 31,  4, 23, 32, 33, 34, 35,  4,  4,  4, 36,  3, 37, 38,\n",
              "       39, 40, 21, 41, 42, 10, 43, 44, 13, 45, 20, 46, 47, 48, 10, 49, 13,\n",
              "       23, 50, 51,  4, 52,  8, 53, 54, 55, 56, 57, 58, 59,  4, 60, 61, 62,\n",
              "       63, 64, 65,  7, 66,  4,  4,  4,  8, 67, 68, 69, 55, 23, 70, 32, 71,\n",
              "       72, 73, 74, 75, 76, 77, 47, 10, 78, 79, 80, 81,  4, 82, 61, 62, 63,\n",
              "       20, 83, 84,  4, 85, 86, 87, 88, 89, 90,  4])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "mQdocfTF4K7Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Turn text datasets into integer datasets\n",
        "\n",
        "Now that we have a numbered vocabulary, we can encode each review as a Tensor of integers representing words.\n",
        "\n",
        "Each input Tensor needs to be the same length. So, first determine the length of the longest review. Then, tokenize the examples and encode them as a lists of integers, with a padding of zeroes to make all examples the same length.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "MUEgNkjckEML",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f5076293-e9e1-4118-d569-439296e844e9"
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 512\n",
        "MAX_LEN = max([len(ex[0].numpy().split(\" \")) for ex in train_data])\n",
        "MAX_LEN"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2470"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "lR0b0Qr-kZm8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorized_train_data = train_data.map(lambda example, label: )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0NOibWNd4UtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_int_encoded_dataset(vocabulary, dataset):\n",
        "\n",
        "  def encode_and_pad(tokenized_text, label):\n",
        "\n",
        "    def helper(tokenized_text):\n",
        "      tokenized_text = tokenized_text.numpy()\n",
        "      result = []\n",
        "      for word in tokenized_text:\n",
        "        result.append(vocabulary[word])\n",
        "      return tf.pad(result, [[0, MAX_LEN - len(result)]], 'CONSTANT')\n",
        "\n",
        "    return tf.py_function(helper, [tokenized_text], tf.int64), label\n",
        "\n",
        "  dataset = dataset.map(tokenize)\n",
        "  dataset = dataset.map(encode_and_pad)\n",
        "  dataset = dataset.shuffle(10 * BATCH_SIZE)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  return dataset\n",
        "\n",
        "train_data = get_int_encoded_dataset(vocabulary, train_data)\n",
        "\n",
        "\n",
        "# THINGS TO TRY\n",
        "# ragged tensor\n",
        "# padded batch\n",
        "# bucket by sequence length\n",
        "# feature columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "67YbWWli8bwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_batch,label_batch = next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJwt5dRS87E8",
        "colab_type": "code",
        "outputId": "d5f19704-6087-4d67-f467-b85b7b30afd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "text_batch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=1753688, shape=(512, 2470), dtype=int64, numpy=\n",
              "array([[  321,    85,   510, ...,     0,     0,     0],\n",
              "       [  421,   325,  4682, ...,     0,     0,     0],\n",
              "       [    0,   187,    85, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [ 1481,    11, 10541, ...,     0,     0,     0],\n",
              "       [  242, 10129,     6, ...,     0,     0,     0],\n",
              "       [ 6043,   245,   372, ...,     0,     0,     0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "tALi4WYq9I86",
        "colab_type": "code",
        "outputId": "980c7e49-63cd-4124-e0e8-3cef12430716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.pcolormesh(text_batch.numpy() != 0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.QuadMesh at 0x7f0547253110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAGbdJREFUeJzt3X9sVWfhx/HPKfSmVjvxdtzOmv2K\nYa6ZFWw6DSzgatkSMOuAUTJJR9Qxt8wSYpxQJ1ETjeWXJAOJbKzLGnCx2XUx/WMpzaJL0HQ12zVN\nWUwISzQEu/ZettFR2sKa8/1jXyrDtvf23nPueZ5z3q+kf7RruU/Pmvt+nuece67juq4rAAAQqJKg\nBwAAAAgyAABGIMgAABiAIAMAYACCDACAAQgyAAAGWBjkgzuOE+TDwyO8cg4AChdokEHMAAAfi0SQ\niR4AwHRZg9zf36/t27dryZIlkqQ77rhDW7du1Y4dOzQ1NaXFixdr3759isVi6u7uVmdnp0pKSrRp\n0yY1Nzf7/gvkwnEcogwAMFpOK+Svfe1rOnjw4PTnP/nJT7R582atWbNGBw4cUDKZ1Lp163T48GEl\nk0mVlpZq48aNuu+++7Ro0SLfBj8f+Z6vJuQAgGLI6yrr/v5+NTY2SpIaGhrU19engYEB1dbWqqKi\nQmVlZaqrq1MqlfJ0sPPlum7BHwAAFENOK+QzZ87oiSee0IULF9Ta2qrx8XHFYjFJUmVlpdLptDKZ\njOLx+PTPxONxpdNpf0ado1xXxYQXABC0rEG+7bbb1NraqjVr1ujs2bPasmWLpqampv/7bDEzLXKm\njQcAgGtl3bKuqqrS2rVr5TiObrnlFt144426cOGCJiYmJEnDw8NKJBJKJBLKZDLTPzcyMqJEIuHf\nyOfJcRxfPwAAKETWIHd3d6ujo0OSlE6ndf78eW3YsEEnTpyQJPX29mrlypVaunSpBgcHNTo6qrGx\nMaVSKdXX1/s7eh9wrhkAEATHzVKTixcv6qmnntLo6KiuXLmi1tZW1dTUaOfOnZqcnFR1dbXa29tV\nWlqqnp4edXR0yHEctbS0qKmpae4HN3BlSVwBAEHIGmRfH9yQIBNhAEDQInGnrmzmmhgQawBAMRDk\nLExZxUcdEyMAYRfZIPMEDwAwSaiDTHQBALYIdZDns91MvAEAQQp1kOcjiueKmYQAgDkIcoTZNAlh\n8gAg7AgyCkYsAaBwBNlyxBAAwoEg54EIAgC8FrogE0sAgI1CF+TrL1Qi0AAAG4QuyNeb7UpiQg0A\nMEnog0x4AQA2KAl6AH5zHMeq19sCAKIp9Cvkq4gy2C0BYLLIBBkwcVLGJAHAVQRZPCkCAIIXuiAT\nVwCAjUIX5EK2JYk5ACAo1gSZWAIAwsyaIBfjghyiDwAIijVBLhSxBQCYLDJB5iUvAACTRSbIsyGK\nAAATBBpk13V9X7kSXACADQINcjG2kU3cqjYJExYAMENot6wJDQDAJlYHmegCAMLC6iB7tR1N2AEA\nQbM6yFcRVACA7UIR5HxWykQcAGCSUAQ5H/ludxNyAIAfIhvkfF0bcuIMAPAKQZ5BrjcsseU1zkwc\nAMB8oQ9yvjEiYgCAYjI+yIQRABAFxgfZj21hIg8AMI3xQfaDLed+54NJBgDYLZJBzgWBAwAUk1VB\nJpIAgLAyPshEGAAQBcYH2c/zvcQeAGAK44Psp1xiT7QBAMUQ6SATWwCAKSId5DC+/GkmTDwAwHyR\nDnJURGXi4TUmMgCKKfRB5kkVAGCD0Ad5ptUhkQYAmCb0QZ5JsbZwCT8AIFcluXzTxMSEVq9erVde\neUVDQ0N65JFHtHnzZm3fvl2XL1+WJHV3d+uhhx5Sc3OzXn75ZV8H7TfXdT35AAAgVzkF+Xe/+50+\n+9nPSpIOHjyozZs366WXXtKtt96qZDKpS5cu6fDhw3rxxRd17NgxdXZ26oMPPvB14H5yHMeKDwBA\neGQN8jvvvKMzZ87o3nvvlST19/ersbFRktTQ0KC+vj4NDAyotrZWFRUVKisrU11dnVKplK8Dn00x\nVqZeraBZgQMArsoa5D179qitrW368/HxccViMUlSZWWl0um0MpmM4vH49PfE43Gl02kfhptdMVaO\nrGABAF6b86KuP/3pT1q2bJluvvnmGf/7bKs0v1ZvrusaEbpcx2HCWGE/dkOAaJgzyK+//rrOnj2r\n119/Xe+++65isZjKy8s1MTGhsrIyDQ8PK5FIKJFIKJPJTP/cyMiIli1b5vlgCwmc109qPEkCALzk\nuDmW5dChQ/rCF76gf/zjH6qvr9eDDz6oX/3qV/rSl76kBx54QA888ID++Mc/asGCBdqwYYOSyaQq\nKirmfnBWkEZgcgEAwZv365C3bdumnTt3qqurS9XV1Vq3bp1KS0v1ox/9SI8++qgcx9EPfvCDrDEu\nBAEBAIRNzitkXx68iDfoyPWxiD0AIAhW3amrkFgSWgCAyawKcj4rakIMALCBVUEmrgCAsLIqyH6d\ncyb0AICgWRVkvwT98ismBAAAglwgYgoA8EKkg0xMAQCmiHSQg96qLgSTCQAIl0gH2WY2TyYAZGfC\nm+kw8S+u0AeZPygAtuL5K1pCH+SgZ5hRxJMIAMxf6IOcCwICAAgaQZb/q2iCDwDIhiAXgenb5kwY\nACB4BBmeTRgIOwDkjyDPgLAAAIotUkEmtAAAU0UqyEGdyw36Bf5MRADAfJEK8lVBBIooAgDmUhL0\nAIJg+lXPAIDoCfUKmVUpAMAWoQ4yK2FvMcEBAP9YH2QiAQAIA+uCTIABAGFkXZDDsA3NpAIAcD3r\ngmwKogoA8FIkg0xMAQCmCX2QiS8AwAahD/L155wJNADARNYHmcACAMLA+iDPdtU1oQYA2CSS97IG\nAMA01q+QZ5PL65VZRQMATBHaIOciyPdHBgDgWqEPMvEDANgg9OeQHccJxe02AQDhFvoV8lU2R5lV\nPgCEX2SCbAriCgCYCUEuEIEFAHgh9OeQ/cY5agCAFyKxQmYVCwAwXSSCbPoKlgkDACASQc4XoQQA\nFAtBngNv3QgAKJZIB5nAAgBMEekgc24ZAGCKSAf5WsQPABAkgvz/vFgtE3UAQL4IsodM3wKH+ZjU\nAdGVNcjj4+Nqa2vT+fPnNTk5qSeffFJ33nmnduzYoampKS1evFj79u1TLBZTd3e3Ojs7VVJSok2b\nNqm5ubkYvwMMRVwAIHeOm+VZ89VXX9W5c+f02GOP6dy5c/re976nuro6rVq1SmvWrNGBAwd00003\nad26dVq/fr2SyaRKS0u1ceNGHT9+XIsWLZr9wS1fURIcAIBXst7Leu3atXrsscckSUNDQ6qqqlJ/\nf78aGxslSQ0NDerr69PAwIBqa2tVUVGhsrIy1dXVKZVK+Tv667iuW9QPAAC8kvM55Icffljvvvuu\njhw5ou9+97uKxWKSpMrKSqXTaWUyGcXj8envj8fjSqfT3o94DtlW3K7rzvg9xBUAELScg/yHP/xB\n//znP/XjH//4EwGbLWYmRm62YNuwdW7i8QQAeCfrlvWpU6c0NDQkSaqpqdHU1JQ+/elPa2JiQpI0\nPDysRCKhRCKhTCYz/XMjIyNKJBI+DdscbI8DALyQNchvvvmmXnjhBUlSJpPRpUuXtGLFCp04cUKS\n1Nvbq5UrV2rp0qUaHBzU6OioxsbGlEqlVF9f7+/oDXD1/ZC9/gAAREvWq6wnJib005/+VENDQ5qY\nmFBra6u+/OUva+fOnZqcnFR1dbXa29tVWlqqnp4edXR0yHEctbS0qKmpae4HJzywFLsWALyWNci+\nPjhBzhkBAIBws+ZOXQQJABBm1gTZlNU0EwMAgB+sCfJVBBEAEEbGB5kAAwCiwPgg+7FVTeQBAKYx\nPsh+MOV8NDAbJo1A9EQyyIXgiRIA4IfIBZmgAgBMFJogE1oAgM1CE+QwnhdmkgEA0RGaIM+FsAEA\nTBeJIM+2eibUAABThDrIBBcAYItQB9nG88pMIgAgmkId5PkghACAIEUqyEQXAGCqSAWZi7sAAKaK\nVJBn4+W5ZuIOAMhHpIJMLAEApioJegDFZONV1wCAaIjUClkiyrZidwNA2EUuyLYgQAAQLUYF2XVd\no1awRBEAUCxGBdmkGEvej2euCQfxB4BoMyrIV4U5TmH+3QAA+TMyyI7jFG37mkACAExg7MueirV9\nbdo2OQAgmoxcIRdbsaPMqhwAcD0rguzH9jVRBACYxNgt62uxrQwACDsrVsheYmUMADCRdUEmqACA\nMLIuyGxfB4OJEAD4y7ogRxUXtgFAuFkRZG45+bEo/a4AEDVWBHmulWG2VSMRAwDYwPggF7pV68U2\nL1EHAPjN+CCbcBFXrmMg3ACAfBkfZJtwC04AQL4iEWTCBQAwXSSCfO3KlTgDAEwU6iATXwCALax4\nc4l8OY5jxEVhAABkY+UKeb4vheKmIgAA01kZZK9Wvayec8PEBQD8Z2WQc0FEAAA2CfU5ZAAAbBHa\nFfJ8tqNzOSfNihsA4KfQBnk+col3LsE28Zy01+NiYgIA/sgpyHv37tVbb72ljz76SI8//rhqa2u1\nY8cOTU1NafHixdq3b59isZi6u7vV2dmpkpISbdq0Sc3NzX6Pv2BeBsbUWJk6LgDAfzlulmfrN954\nQx0dHTp69Kjef/99rV+/XsuXL9eqVau0Zs0aHThwQDfddJPWrVun9evXK5lMqrS0VBs3btTx48e1\naNGi2R/cwBUl/hdBBwD/Zb2o6+6779YzzzwjSbrhhhs0Pj6u/v5+NTY2SpIaGhrU19engYEB1dbW\nqqKiQmVlZaqrq1MqlfJ39AVyXZePHD4AAP7LumW9YMEClZeXS5KSyaRWrVqlv/71r4rFYpKkyspK\npdNpZTIZxePx6Z+Lx+NKp9M+DdsbrNCjiUkGABPl/LKn1157TclkUj/72c8+8fXZntx40oOprt5S\n9doPAAhaThd1nTx5UkeOHNHzzz+viooKlZeXa2JiQmVlZRoeHlYikVAikVAmk5n+mZGRES1btsy3\ngWN2TIYAwD5ZV8gffvih9u7dq2effXb6Aq0VK1boxIkTkqTe3l6tXLlSS5cu1eDgoEZHRzU2NqZU\nKqX6+np/Rz9PQZ+L5ZwvAGA2Wa+y7urq0qFDh3T77bdPf2337t3atWuXJicnVV1drfb2dpWWlqqn\np0cdHR1yHEctLS1qamqa+8EN2yokZgCAoGQNsq8PXoQgE1kAgA1Cf6eumaJPpAEApgl1kAkvAMAW\noQ4yW+IAAFuEOsjFYMqFaUwMAMBuBHkGxA0AUGyRCTKRBQCYzIggu67/7yU8279PqAEAJsj5XtZ+\nCuo8LDEGAJjCiBVyMRFhAICJIhdkU66KNgmTFAAIXiSCTHAAAKYzOsiEFAAQFUYH2avtZcIOADCd\n0UG+HmEFAISVVUHmgqzsmLQAgJ2sCrLJCCEAoBAE2SNhXr0z2QAA/xFkjxEvAEA+CLKIKAAgeARZ\n4d5uzhWTEgAIFkGeAXECABRb6IM8n7d2vPq9vFUjAKDYQh/kuWI8U2CJLgAgCFYEmUgCAMLOiiAX\n+6IrJgAAgGKzIsjFls8EgIgDAApBkPNAfAEAXiPIIrAAgOBFKsiEFwBgqkgFOdu5YYINAAhKSdAD\nMMlcNwUBAMBPkVoh54ooRxM7JACCRJAtRTwAIFysCDLxAQCEnRVBZgs5OyYtAGA3K4KcDwIFALBJ\naIMcllU1EwsAiIbQBvl6hA0AYLLIBNmPFTORBwB4JXRBJpIAABuF6k5dxBgAYKtQrZCv35Ym0AAA\nW1gfZKILAAgD64M834u1XNflXZ8AAMaxPsi5ujayBBcAYJrIBNnvG4UQeQBAIawNMgEEAISJtUHm\nRh8AgDCxNsh+MO3+10wQACA6QhlkQgYAsE1Od+o6ffq0Vq9erePHj0uShoaG9Mgjj2jz5s3avn27\nLl++LEnq7u7WQw89pObmZr388sv+jToLx3GyfgAAYJKsQb506ZJ++ctfavny5dNfO3jwoDZv3qyX\nXnpJt956q5LJpC5duqTDhw/rxRdf1LFjx9TZ2akPPvjAt4G7rlvQBwAAJska5FgspqNHjyqRSEx/\nrb+/X42NjZKkhoYG9fX1aWBgQLW1taqoqFBZWZnq6uqUSqV8GzirXwBAmGQ9h7xw4UItXPjJbxsf\nH1csFpMkVVZWKp1OK5PJKB6PT39PPB5XOp32eLi5mS3KrIwBAKYq+KKu2SJnYvyivHo28f8HAOC/\n8gpyeXm5JiYmVFZWpuHhYSUSCSUSCWUymenvGRkZ0bJlyzwbaNAIGgDAT3m9H/KKFSt04sQJSVJv\nb69WrlyppUuXanBwUKOjoxobG1MqlVJ9fb2ngy0EF4EBAEzmuFlqc+rUKe3Zs0fnzp3TwoULVVVV\npf3796utrU2Tk5Oqrq5We3u7SktL1dPTo46ODjmOo5aWFjU1Nc394Hm8UxMAAGGUNci+PnhA53QJ\nOwDANKG8U1c2Ub64ay5MVAAgOJEIMqEBAJjO2iATWQBAmFgbZK+3nQk8ACBI1gY5X4QXAGAiK4NM\nVAEAYWNlkE27SpoJAgCgUFYG2TSmTRBsxuQGQFRFPsgEAABggsgHmbdqBACYINRBJqoAAFuEOsi5\nntsl3ACAoFkZZAIKAAgbK4PMyhcAEDZWBvlaruvOGmhejlQcTHwAoHDWB7nQ6BITAIAJrA9yoVhF\noxiY+AHIxsogz7VNXcjP86QJAAiK4wZYIVan+WHiAADhY+UK2UREEgBQCIKcJwIMAPASQZ4nQgwA\n8INVQSaGAICwsirIJlwExqQAAOAHq4IcJEIMAPATQb4G0QUABIUgX2OuLXFiDQDwU2SCTFABACaL\nTJBNuCDsekwSAABXRSbIQSK8AIBsIhtkIgkAMElkg8wFXAAAk0Q2yHPx6nwzYQcA5Cr0QSaKAAAb\nWBNkwgoACDNrglzMly0RfwBAsVkT5GLyOv6u6xZlQsFEAgDsRZCLYKYYE08AwLUiE2QCCAAwmZVB\nJq4AgLCxMsgm3pc6F0wkAACzsTLIfiKaAIAgEOTrcJcuAEAQIhdkQgkAMFHkgpzvCpiQAwD8FIkg\ne3FjDu4UBgDwUySCPJ+YEkMAQBCsCjKxBACElVVBtvX1x7liwgEA0eV5kH/9619rYGBAjuPo6aef\n1le+8hWvH2LeCB0AwHSeBvnvf/+7/v3vf6urq0vvvPOOnn76aXV1dXn5EHkJ+8q6UExYACB4JV7+\nY319fVq9erUk6Ytf/KIuXLigixcvevkQyJPrurN+AACC5+kKOZPJ6K677pr+PB6PK51O6zOf+YyX\nD2MEQgYA8JKvF3VlixZRAwDgY55uWScSCWUymenPR0ZGtHjxYi8fAgCAUPI0yPfcc49OnDghSXr7\n7beVSCRCuV0NAIDXPN2yrqur01133aWHH35YjuPo5z//uZf/PAAAoeW4nMgFACBwnm5ZAwCA/BBk\nAAAMENi9rE28xaat+vv7tX37di1ZskSSdMcdd2jr1q3asWOHpqamtHjxYu3bt0+xWEzd3d3q7OxU\nSUmJNm3apObm5oBHb4fTp0/rySef1He+8x21tLRoaGgo5+N75coVtbW16T//+Y8WLFig9vZ23Xzz\nzUH/Ssa6/li3tbXp7bff1qJFiyRJjz76qO69916OdYH27t2rt956Sx999JEef/xx1dbW8jcdNDcA\n/f397ve//33XdV33zJkz7qZNm4IYRmi88cYb7rZt2z7xtba2NvfVV191Xdd1f/Ob37i///3v3bGx\nMff+++93R0dH3fHxcfdb3/qW+/777wcxZKuMjY25LS0t7q5du9xjx465rju/4/vKK6+4v/jFL1zX\ndd2TJ0+627dvD+x3Md1Mx3rnzp3un//85//5Po51/vr6+tytW7e6ruu67733nvuNb3yDv2kDBLJl\nzS02/dff36/GxkZJUkNDg/r6+jQwMKDa2lpVVFSorKxMdXV1SqVSAY/UfLFYTEePHlUikZj+2nyO\nb19fn+677z5J0ooVKzjmc5jpWM+EY12Yu+++W88884wk6YYbbtD4+Dh/0wYIJMiZTEaf+9znpj+/\neotN5O/MmTN64okn9O1vf1t/+9vfND4+rlgsJkmqrKxUOp1WJpNRPB6f/hmOe24WLlyosrKyT3xt\nPsf32q+XlJTIcRxdvny5eL+ARWY61pJ0/PhxbdmyRT/84Q/13nvvcawLtGDBApWXl0uSksmkVq1a\nxd+0AYx4P2SXV14V5LbbblNra6vWrFmjs2fPasuWLZqampr+77MdX467N+Z7fDnu8/Pggw9q0aJF\nqqmp0XPPPaff/va3+upXv/qJ7+FY5+e1115TMpnUCy+8oPvvv3/66/xNByOQFTK32PRWVVWV1q5d\nK8dxdMstt+jGG2/UhQsXNDExIUkaHh5WIpGY8bhn2xrEzMrLy3M+volEYnon4sqVK3Jdd3olguyW\nL1+umpoaSdI3v/lNnT59mmPtgZMnT+rIkSM6evSoKioq+Js2QCBB5hab3uru7lZHR4ckKZ1O6/z5\n89qwYcP0Me7t7dXKlSu1dOlSDQ4OanR0VGNjY0qlUqqvrw9y6NZasWJFzsf3nnvuUU9PjyTpL3/5\ni77+9a8HOXTrbNu2TWfPnpX08bn7JUuWcKwL9OGHH2rv3r169tlnp69e5286eIHdqWv//v168803\np2+xeeeddwYxjFC4ePGinnrqKY2OjurKlStqbW1VTU2Ndu7cqcnJSVVXV6u9vV2lpaXq6elRR0eH\nHMdRS0uLmpqagh6+8U6dOqU9e/bo3LlzWrhwoaqqqrR//361tbXldHynpqa0a9cu/etf/1IsFtPu\n3bv1+c9/Puhfy0gzHeuWlhY999xz+tSnPqXy8nK1t7ersrKSY12Arq4uHTp0SLfffvv013bv3q1d\nu3bxNx0gbp0JAIABuFMXAAAGIMgAABiAIAMAYACCDACAAQgyAAAGIMgAABiAIAMAYACCDACAAf4P\nQv1SNaZiVrsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "FKIow6eYlnSn",
        "colab_type": "code",
        "outputId": "fecd1ff0-f8a5-4b46-dca2-2010f853f192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1095
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_model(input_dim, embedding_dim=50, hidden_units=[100]):\n",
        "  \"\"\"Create a Keras Sequential model with layers.\n",
        "\n",
        "  Args:\n",
        "    input_dim: (int) Input dimensions for input layer.\n",
        "    embedding_dim: (int) Embedding dimension for embedding layer.\n",
        "    hidden_units: [int] the layer sizes of the DNN (input layer first)\n",
        "\n",
        "  Returns:\n",
        "    A Keras model.\n",
        "  \"\"\"\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(input_dim=input_dim,\n",
        "                                      output_dim=embedding_dim,\n",
        "                                      input_length=MAX_LEN))\n",
        "  # convolutional layer or RNN\n",
        "  model.add(tf.keras.layers.GlobalMaxPool1D())\n",
        "  for units in hidden_units:\n",
        "    model.add(tf.keras.layers.Dense(units, activation=tf.keras.backend.relu))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = get_model(len(vocabulary))\n",
        "model.fit(train_data, epochs=10)\n",
        "\n",
        "test_data = get_indexed_dataset(vocabulary, test_files)\n",
        "model.evaluate(test_data)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-alpha0 in /usr/local/lib/python2.7/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.1.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.post1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==2.0.0-alpha0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (5.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.0.1)\n",
            "345418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0321 22:29:00.914769 140374431422336 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/script_ops.py:476: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 38s 769ms/step - loss: 0.6856 - accuracy: 0.5904\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 36s 744ms/step - loss: 0.5781 - accuracy: 0.8380\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 37s 749ms/step - loss: 0.3396 - accuracy: 0.8825\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 37s 750ms/step - loss: 0.1999 - accuracy: 0.9308\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 39s 800ms/step - loss: 0.1140 - accuracy: 0.9659\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 38s 771ms/step - loss: 0.0593 - accuracy: 0.9877\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 37s 765ms/step - loss: 0.0296 - accuracy: 0.9963\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 37s 757ms/step - loss: 0.0152 - accuracy: 0.9990\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 36s 743ms/step - loss: 0.0085 - accuracy: 0.9998\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 36s 742ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "     49/Unknown - 24s 496ms/step - loss: 0.3709 - accuracy: 0.8570"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37087987332927935, 0.85696]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    }
  ]
}