{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DweYe9FcbMK_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 The TensorFlow Authors.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AVV2e0XKbJeX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sUtoed20cRJJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load text with tf.data"
      ]
    },
    {
      "metadata": {
        "id": "1ap_W4aQcgNT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/alpha/tutorials/load_data/text\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/text.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "metadata": {
        "id": "NWeQAo0Ec_BL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorial provides an example of how to use `tf.data.TextLineDataset` to load examples from text files. `TextLineDataset` is designed to create a dataset from a text file, in which each example is a line of text from the original file. This is potentially useful for any text data that is primarily line-based (for example, poetry or dialogue).\n",
        "\n",
        "In this tutorial, we'll use three different English translations of the same work, Homer's Illiad, and train a model to identify the translator given a single line of text."
      ]
    },
    {
      "metadata": {
        "id": "fgZ9gjmPfSnK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ]
    },
    {
      "metadata": {
        "id": "ahCrZgPtYv5H",
        "colab_type": "code",
        "outputId": "c5070a46-d3c9-4f6f-d1f7-e96de9d3aff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/84/970bfb6eb04360a675627a38962127f0f5302ac1fd3ac4ad4f5d1befc9b7/tensorflow-2.0.0a0-cp27-cp27mu-manylinux1_x86_64.whl (79.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 79.9MB 146kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/4f/369f43db86ee663826dc4a7cce7e18b3f9c58c8defc9e78368230b015d2b/tb_nightly-1.14.0a20190301-py2-none-any.whl (3.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.0MB 7.5MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.2 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/7f/0471cd7d94df22a09e92e06b5722b58c9fca71c6617347fdbf4b88206a0d/google_pasta-0.1.4-py2-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 17.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.post1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K    100% |████████████████████████████████| 419kB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==2.0.0-alpha0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (5.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.15.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.1)\n",
            "Installing collected packages: tb-nightly, google-pasta, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.13.1\n",
            "    Uninstalling tensorflow-1.13.1:\n",
            "      Successfully uninstalled tensorflow-1.13.1\n",
            "Successfully installed google-pasta-0.1.4 tb-nightly-1.14.0a20190301 tensorflow-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "baYFZMW_bJHh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function \n",
        "\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWVWjyIkffau",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The texts of the three translations are from Project Gutenberg:\n",
        "\n",
        " - [William Cowper](http://www.gutenberg.org/cache/epub/16452/pg16452.txt)\n",
        " \n",
        " - [Edward, Earl of Derby](http://www.gutenberg.org/cache/epub/6150/pg6150.txt)\n",
        " \n",
        "- [Samuel Butler](http://www.gutenberg.org/cache/epub/2199/pg2199.txt)\n",
        "\n",
        "The text files used in this tutorial have undergone some typical preprocessing tasks, mostly removing stuff — document header and footer, line numbers, chapter titles. Download these lightly munged files locally. "
      ]
    },
    {
      "metadata": {
        "id": "hRI0ZlhTgQ8Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DIRECTORY_URL = 'https://s3.amazonaws.com/illiad/'\n",
        "FILE_NAMES = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
        "\n",
        "for name in FILE_NAMES:\n",
        "  r = requests.get(\"\".join([DIRECTORY_URL, name]), allow_redirects=True)\n",
        "  with open(name, 'w') as f:\n",
        "    f.write(r.content)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "54Dv7mCrf9Yw",
        "colab_type": "code",
        "outputId": "9d6f1e05-8a93-4a20-9a16-cb6c33bf63fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "butler.txt  cowper.txt\tderby.txt  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q3sDy6nuXoNp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load text into datasets\n",
        "\n",
        "Iterate through the files, loading each one into its own dataset.\n",
        "\n",
        "Each example needs to be labeled individually labeled, so use `tf.data.Dataset.map` to apply a labeler function to each one. This will iterate over every example in the dataset, returning (`example, label`) pairs."
      ]
    },
    {
      "metadata": {
        "id": "K0BjCOpOh7Ch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_labeler(label_int):\n",
        "  \"\"\"Returns a labeler function initialized with a specfic label.\"\"\"\n",
        "  label_tensor = tf.cast(label_int, tf.int64)\n",
        "\n",
        "  def labeler(example):\n",
        "    \"\"\"Returns a labeled example.\"\"\"\n",
        "    return example, label_tensor\n",
        "  \n",
        "  return labeler\n",
        "\n",
        "labeled_data_sets = []\n",
        "for i, file_name in enumerate(FILE_NAMES):\n",
        "  lines_dataset = tf.data.TextLineDataset(file_name)\n",
        "  labeler_function = get_labeler(i)\n",
        "  labeled_dataset = lines_dataset.map(labeler_function)\n",
        "  labeled_data_sets.append(labeled_dataset)\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8PHK5J_cXE5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Combine these labeled datasets into a single dataset, and shuffle it.\n"
      ]
    },
    {
      "metadata": {
        "id": "Qd544E-Sh63L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_labeled_data = labeled_data_sets[0].concatenate(labeled_data_sets[1])\n",
        "all_labeled_data = all_labeled_data.concatenate(labeled_data_sets[2])\n",
        "\n",
        "all_labeled_data = all_labeled_data.shuffle(\n",
        "                    tf.cast(50000, tf.int64), \n",
        "                    reshuffle_each_iteration=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r4JEHrJXeG5k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can use `tf.data.Dataset.take` and `print` to see what the `(example, label)` pairs look like. The `numpy` property shows each Tensor's value."
      ]
    },
    {
      "metadata": {
        "id": "gywKlN0xh6u5",
        "colab_type": "code",
        "outputId": "949d132f-cf26-48d5-f40f-07be7883a2b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "for ex in all_labeled_data.take(5):\n",
        "  print(ex)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: id=69, shape=(), dtype=string, numpy='Hath Agamemnon kindled, King of men.'>, <tf.Tensor: id=70, shape=(), dtype=int64, numpy=1>)\n",
            "(<tf.Tensor: id=73, shape=(), dtype=string, numpy='cry that rent the air. Then Jove the lord of thunder sent the blast of'>, <tf.Tensor: id=74, shape=(), dtype=int64, numpy=2>)\n",
            "(<tf.Tensor: id=77, shape=(), dtype=string, numpy='shield of ox-hides covered with plates of bronze--and his gleaming'>, <tf.Tensor: id=78, shape=(), dtype=int64, numpy=2>)\n",
            "(<tf.Tensor: id=81, shape=(), dtype=string, numpy='Unmoved with anger? Such are day by day'>, <tf.Tensor: id=82, shape=(), dtype=int64, numpy=0>)\n",
            "(<tf.Tensor: id=85, shape=(), dtype=string, numpy='Whole cities oft, though reverenced but by few.'>, <tf.Tensor: id=86, shape=(), dtype=int64, numpy=0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5rrpU2_sfDh0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encode text lines as numbers\n",
        "\n",
        "Machine Learning models work on numbers, not words, so the string values need to be converted into lists of numbers. To do that, map each unique word to a unique integer.\n",
        "\n",
        "### Build Vocabulary\n",
        "\n",
        "First, build a vocabulary by tokenizing the text into a collection of individual unique words. There are a few ways to do this in both TensorFlow and Python. For this tutorial:\n",
        "\n",
        "1. Iterate over each example's `numpy` value.\n",
        "2. Use `tfds.features.text.Tokenizer` to split it into tokens.\n",
        "3. Collect these tokens into a Python set, to remove duplicates.\n",
        "4. Get the size of the vocabulary for later use."
      ]
    },
    {
      "metadata": {
        "id": "YkHtbGnDh6mg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6e1b8402-d1e5-48dd-8fea-e0d6fd9342e9"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.features.text.Tokenizer()\n",
        "vocabulary_set = set()\n",
        "for text_tensor, _ in all_labeled_data:\n",
        "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  vocabulary_set.update(some_tokens)\n",
        "\n",
        "\n",
        "vocab_size = len(vocabulary_set)\n",
        "vocab_size  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17178"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "0W35VJqAh9zs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode examples\n",
        "\n",
        "Create an encoder by passing the `vocabulary_set` to `tfds.features.text.TokenTextEncoder`. The encoder's `encode` method takes in a string of text and returns a list of integers."
      ]
    },
    {
      "metadata": {
        "id": "gkxJIVAth6j0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder = tfds.features.text.TokenTextEncoder(vocabulary_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v6S5Qyabi-vo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can try this on a single line to see what the output looks like."
      ]
    },
    {
      "metadata": {
        "id": "jgxPZaxUuTbk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b19e7570-a65c-4f6e-ef5a-a2e6592ba30d"
      },
      "cell_type": "code",
      "source": [
        "example_text = next(iter(all_labeled_data))[0].numpy()\n",
        "example_text"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hath Agamemnon kindled, King of men.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "XoVpKR3qj5yb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "81827069-89bd-4997-a8ff-ea8d00b12736"
      },
      "cell_type": "code",
      "source": [
        "encoded_example = encoder.encode(example_text)\n",
        "encoded_example"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8918, 15046, 7574, 11392, 14915, 8697]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "p9qHM0v8k_Mg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now run the encoder on the dataset by wrapping it in `tf.py_function` and  passing that to the dataset's `map` method."
      ]
    },
    {
      "metadata": {
        "id": "HcIQ7LOTh6eT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encode(text_tensor, label):\n",
        "  encoded_text = encoder.encode(text_tensor.numpy())\n",
        "  return encoded_text, label\n",
        "\n",
        "def encode_map_fn(text, label):\n",
        "  return tf.py_function(\n",
        "    encode, inp=[text, label], Tout=(tf.int64, tf.int64)\n",
        "  )\n",
        "\n",
        "all_encoded_data = all_labeled_data.map(encode_map_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSFgMNsnaBqS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MAX_LINE_LEN = max([tf.size(ex[0]) for ex in all_encoded_data]).numpy()\n",
        "#MAX_LINE_LEN = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qMGoD2EubPJr",
        "colab_type": "code",
        "outputId": "e55076dd-5d6d-4662-f58f-8df9f51f7ab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#MAX_LINE_LEN"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "_YZToSXSm0qr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Split the dataset into text and train batches\n",
        "\n",
        "Use `tf.data.Dataset.take` and `tf.data.Dataset.skip` to create a small test dataset and a larger training set.\n",
        "\n",
        "Before being passed into the model, the datasets need to be batched. Typically, the examples inside of a batch need to be the same size and shape. But, the examples in these datasets are not all the same size — each line of text had a different number of words. So use `tf.data.Dataset.padded_batch` (instead of `batch`) to pad the examples to the same size."
      ]
    },
    {
      "metadata": {
        "id": "r-rmbijQh6bf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_data = all_encoded_data.take(5000).padded_batch(50, padded_shapes=([-1],[]))\n",
        "train_data = all_encoded_data.skip(5000).padded_batch(50, padded_shapes=([-1],[]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xdz7SVwmqi1l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, `test_data` and `train_data` are not collections of (`example, label`) pairs, but collections of batches. Each batch is a pair of (*many examples*, *many labels*) represented as arrays.\n",
        "\n",
        "To illustrate:"
      ]
    },
    {
      "metadata": {
        "id": "kMslWfuwoqpB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "26f7f668-bc24-438b-b897-f5e03044e0ac"
      },
      "cell_type": "code",
      "source": [
        "padded_example = next(iter(test_data))\n",
        "\n",
        "# a bunch of labels\n",
        "padded_example[1]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=150049, shape=(50,), dtype=int64, numpy=\n",
              "array([1, 2, 2, 0, 0, 1, 1, 0, 2, 2, 1, 0, 2, 0, 0, 1, 1, 2, 1, 2, 2, 0,\n",
              "       0, 1, 0, 2, 1, 0, 0, 2, 0, 2, 0, 2, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1,\n",
              "       1, 2, 0, 1, 2, 2])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "AFLV89xfrrIh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6bfd9f41-c247-42cb-bdd0-9c9d8d7d45ce"
      },
      "cell_type": "code",
      "source": [
        "# a single text line, out of many\n",
        "# notice the zero values padding out the list\n",
        "padded_example[0].numpy()[0]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8918, 15046,  7574, 11392, 14915,  8697,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "K8SUhGFNsmRi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pkTBUVO4h6Y5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_size = vocab_size + 1\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dimension, 64))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "for units in [64, 64]:\n",
        "  model.add(tf.keras.layers.Dense(units, activation=tf.keras.backend.relu))\n",
        "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aLtO33tNh6V8",
        "colab_type": "code",
        "outputId": "05f9ff83-388d-442c-b789-be3b24a61336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(train_data, epochs=3, validation_data=test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "893/893 [==============================] - 65s 72ms/step - loss: 0.5058 - accuracy: 0.7554\n",
            "Epoch 2/3\n",
            "893/893 [==============================] - 64s 72ms/step - loss: 0.2974 - accuracy: 0.8660\n",
            "Epoch 3/3\n",
            "893/893 [==============================] - 63s 71ms/step - loss: 0.2203 - accuracy: 0.9035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdeaef15b90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "KTPCYf_Jh6TH",
        "colab_type": "code",
        "outputId": "91540b33-7406-4f8e-83e9-96d3a1e35d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(test_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    100/Unknown - 6s 57ms/step - loss: 0.4152 - accuracy: 0.8298"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4152007557451725, 0.8298]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "pr05omK9h6Qd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YmCIA-vdh6N1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u3G3I-z6h6K7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A4UFth9mh6IV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yunx9cTJh6Fv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aCMTI-rRh6C7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oDYpm_B1h6AG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ih0CBbN3hv_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make text files available to Python\n",
        "\n",
        "In the local directory, examples are split into `train` and `test` directories. Within each of those, positive reviews will be in a directory called `pos`, and negative ones in a directory called `neg`.\n",
        "\n",
        "Your text data is probably organized differently than this, and may be in a database or other format. The important thing to notice in this step is making the text files available in a Python iterable. In this example, the iterable is a list of file names."
      ]
    },
    {
      "metadata": {
        "id": "v_Hts93IiW-P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_files = [\n",
        "    os.path.join(path, 'train', label, '*') for label in ['pos', 'neg']\n",
        "]\n",
        "test_files = [\n",
        "    os.path.join(path, 'test', label, '*') for label in ['pos', 'neg']\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wWnPL8gXpoJg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create TensorFlow datasets\n",
        "\n",
        "We need to turn a bunch of files into labeled data. \n",
        "\n",
        "The original dataTo do this:\n",
        "\n",
        "1. Use `tf.data.Dataset.list_files` to create a Dataset of file names.\n",
        "2. Use `tf.data.Dataset.flat_map` to iterate through each file name and:\n",
        "\n",
        "  a. Label the item `1` for positive or `0` for negative.\n",
        "  \n",
        "  b. Load the text from the file with `tf.data.TextLineDataset`.\n",
        "  \n",
        "  c. Combine the label with the text data using `tf.data.Dataset.zip`.\n",
        "\n",
        "Apply this process to both the training data files and the test data files.\n"
      ]
    },
    {
      "metadata": {
        "id": "tNvmhqWep66H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_labeled_dataset(patterns):\n",
        "\n",
        "  files = tf.data.Dataset.list_files(patterns).shuffle(len(patterns))\n",
        "  \n",
        "  # Maps a filename to a dataset that produces (review, sentiment) pair.\n",
        "  def flat_map_fn(filename):\n",
        "    label = tf.data.Dataset.from_tensors(\n",
        "        tf.cast(tf.strings.regex_full_match(filename, '^.*pos.*$'), tf.float64))\n",
        "    return tf.data.Dataset.zip((tf.data.TextLineDataset(filename), label))\n",
        "\n",
        "  \n",
        "  return files.flat_map(flat_map_fn)\n",
        "\n",
        "train_data = get_labeled_dataset(train_files)\n",
        "test_data = get_labeled_dataset(test_files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDswcAqo0Ad4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = tfds.features.text.Tokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zz7tkaHA0AbB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ex = next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7c-ea6WI0AX5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized_ex = tokenizer.tokenize(ex[0].numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mg8bXSUE0ASP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create vocabulary\n",
        "vocabulary_list = []\n",
        "for text_tensor, label_tensor in train_data.concatenate(test_data):\n",
        "  some_tokens = tokenizer.tokenize(text_tensor.numpy())\n",
        "  vocabulary_list = vocabulary_list + some_tokens\n",
        "  \n",
        "len(vocabulary_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3S1fXu9v0APb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename = next(iter(train_files))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GsL7vEaqvLlr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E1ap6tDG0AMQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tl_ds = tf.data.TextLineDataset('/root/.keras/datasets/aclImdb/train/pos/0_9.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUdUoxlf0ABu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for text_line in tl_ds.take(5):\n",
        "  print(text_line.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sLLp0vIEz_6V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5EKG4cN2z_xt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GMJxfbyhidID",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build a vocabulary\n",
        "\n",
        "There are many ways to represent text data for input into a machine learning model. For this tutorial, the text of each review will be represented as a list of integers, with each integer representing a single unique word.\n",
        "\n",
        "The first step to doing this is to create a map of unique words to integers. This mapping will be called the `vocabulary`, and will be a Python dictionary with words as keys and integer Tensors as values.\n",
        "\n",
        "To build the vocabulary:\n",
        "\n",
        "1. Use `tf.data.Dataset.map` to tokenize each example:\n",
        "\n",
        "  a. Remove punctuation and other non-word characters from the text examples.\n",
        "  \n",
        "  b. Split them into arrays of tokens (word-like substrings).\n",
        "\n",
        "2. Create a new dataset in which each element is a token from the text data.\n",
        "\n",
        "3. Remove duplicates eith `tf.data.experimental.unique`.\n",
        "\n",
        "4. Assign a unique integer to each token with `tf.data.experimental.Counter`.\n",
        "\n",
        "5. Create a Python dictionary in which the keys are tokens and the values are integer tensors.\n",
        "\n",
        "6. Pass the keys (words) and values (integers) to `tf.lookup.KeyValueTensorInitializer`.\n",
        "\n",
        "7. Use the initializer to create a `tf.lookup.StaticVocabularyTable`.\n",
        "\n",
        "The `StaticVocabularyTable` holds the mapping of word tokens to integers, and also handles the encoding of word lists to integer lists."
      ]
    },
    {
      "metadata": {
        "id": "3XWR8cdtzxQi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  # Replace line breaks with spaces.\n",
        "  text = tf.strings.regex_replace(text, r'\\<br \\/\\>', ' ')\n",
        "  # Replace punctuation with spaces.\n",
        "  text = tf.strings.regex_replace(text, r'\\W', ' ')\n",
        "  # Turn the single long string into a list of strings.\n",
        "  tokens = tf.strings.split([text], sep=\" \").values\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def get_vocabulary_table(dataset):\n",
        "  # Tokenize the example text and drop the label.\n",
        "  dataset = dataset.map(lambda text, label: tokenize(text))\n",
        "  # Gather all the word tensors into a single dataset.\n",
        "  dataset = dataset.flat_map(tf.data.Dataset.from_tensor_slices)\n",
        "  # Remove duplicates.\n",
        "  dataset = dataset.apply(tf.data.experimental.unique())\n",
        "  # Assign an integer to each token.\n",
        "  dataset = tf.data.Dataset.zip((dataset, tf.data.experimental.Counter()))\n",
        "  # Turn (word, integer) pairs into a dict, so they can be passed easily into initializer.\n",
        "  vocabulary_dict = {word.numpy():index for word, index in iter(dataset)}\n",
        "  \n",
        "  vocabulary_table_initializer = tf.lookup.KeyValueTensorInitializer(\n",
        "      vocabulary_dict.keys(), \n",
        "      vocabulary_dict.values(), \n",
        "      tf.string\n",
        "  )\n",
        "  \n",
        "  return tf.lookup.StaticVocabularyTable(vocabulary_table_initializer, 1)\n",
        "  \n",
        "vocabulary_table = get_vocabulary_table(train_data.concatenate(test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q56pyygvo3qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `tf.lookup.StaticVocabularyTable` has a method `lookup`, which converts a list of words into a list of integers.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "metadata": {
        "id": "a3fibNrdbwvw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocabulary_table.lookup(tf.constant(['I', 'loved', 'this', 'movie']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mQdocfTF4K7Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Turn text datasets into integer datasets\n",
        "\n",
        "Now that we have a numbered vocabulary, we can encode each review as a Tensor of integers.\n",
        "\n",
        "Each input Tensor needs to be the same length. So, first determine the length of the longest review. Then, tokenize the examples and encode them as a lists of integers, with a padding of zeroes to make all examples the same length.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "j9dCWyGDrzka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TDlJPejqsaJP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4CE8szR_rzZQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized_train_data = train_data.map(lambda text, label: (tokenize(text), label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0NOibWNd4UtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_int_encoded_dataset(vocabulary, dataset):\n",
        "\n",
        "  def encode_and_pad(tokenized_text, label):\n",
        "\n",
        "    def helper(tokenized_text):\n",
        "      tokenized_text = tokenized_text.numpy()\n",
        "      result = []\n",
        "      for word in tokenized_text:\n",
        "        result.append(vocabulary[word])\n",
        "      return tf.pad(result, [[0, MAX_LEN - len(result)]], 'CONSTANT')\n",
        "\n",
        "    return tf.py_function(helper, [tokenized_text], tf.int64), label\n",
        "\n",
        "  dataset = dataset.map(tokenize)\n",
        "  dataset = dataset.map(encode_and_pad)\n",
        "  dataset = dataset.shuffle(10 * BATCH_SIZE)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  return dataset\n",
        "\n",
        "train_data = get_int_encoded_dataset(vocabulary, train_data)\n",
        "\n",
        "\n",
        "# THINGS TO TRY\n",
        "# ragged tensor\n",
        "# padded batch\n",
        "# bucket by sequence length\n",
        "# feature columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "67YbWWli8bwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_batch,label_batch = next(iter(train_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJwt5dRS87E8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tALi4WYq9I86",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.pcolormesh(text_batch.numpy() != 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKIow6eYlnSn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_model(input_dim, embedding_dim=50, hidden_units=[100]):\n",
        "  \"\"\"Create a Keras Sequential model with layers.\n",
        "\n",
        "  Args:\n",
        "    input_dim: (int) Input dimensions for input layer.\n",
        "    embedding_dim: (int) Embedding dimension for embedding layer.\n",
        "    hidden_units: [int] the layer sizes of the DNN (input layer first)\n",
        "\n",
        "  Returns:\n",
        "    A Keras model.\n",
        "  \"\"\"\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(input_dim=input_dim,\n",
        "                                      output_dim=embedding_dim,\n",
        "                                      input_length=MAX_LEN))\n",
        "  # convolutional layer or RNN\n",
        "  model.add(tf.keras.layers.GlobalMaxPool1D())\n",
        "  for units in hidden_units:\n",
        "    model.add(tf.keras.layers.Dense(units, activation=tf.keras.backend.relu))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = get_model(len(vocabulary))\n",
        "model.fit(train_data, epochs=10)\n",
        "\n",
        "test_data = get_indexed_dataset(vocabulary, test_files)\n",
        "model.evaluate(test_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}