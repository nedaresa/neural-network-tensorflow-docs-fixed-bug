{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "FKIow6eYlnSn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1095
        },
        "outputId": "fecd1ff0-f8a5-4b46-dca2-2010f853f192"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "!pip install tensorflow==2.0.0-alpha0\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "DATA_URL = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "BATCH_SIZE = 512\n",
        "MAX_LEN = 256\n",
        "NUM_REVIEWS = 25000\n",
        "\n",
        "\n",
        "def get_model(input_dim, embedding_dim=50, hidden_units=[100]):\n",
        "  \"\"\"Create a Keras Sequential model with layers.\n",
        "\n",
        "  Args:\n",
        "    input_dim: (int) Input dimensions for input layer.\n",
        "    embedding_dim: (int) Embedding dimension for embedding layer.\n",
        "    hidden_units: [int] the layer sizes of the DNN (input layer first)\n",
        "\n",
        "  Returns:\n",
        "    A Keras model.\n",
        "  \"\"\"\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(input_dim=input_dim,\n",
        "                                      output_dim=embedding_dim,\n",
        "                                      input_length=MAX_LEN))\n",
        "  model.add(tf.keras.layers.GlobalMaxPool1D())\n",
        "  for units in hidden_units:\n",
        "    model.add(tf.keras.layers.Dense(units, activation=tf.keras.backend.relu))\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "def get_labeled_dataset(patterns):\n",
        "\n",
        "  # Maps a filename to a dataset that produces (review, sentiment) pair.\n",
        "  def flat_map_fn(filename):\n",
        "    label = tf.data.Dataset.from_tensors(\n",
        "        tf.cast(tf.strings.regex_full_match(filename, '^.*pos.*$'), tf.float64))\n",
        "    return tf.data.Dataset.zip((tf.data.TextLineDataset(filename), label))\n",
        "\n",
        "  files = tf.data.Dataset.list_files(patterns).shuffle(NUM_REVIEWS)\n",
        "  return files.flat_map(flat_map_fn)\n",
        "\n",
        "\n",
        "def tokenize(line, label):\n",
        "  # Replace line breaks with spaces.\n",
        "  line = tf.strings.regex_replace(line, r'\\<br \\/\\>', ' ')\n",
        "  # Replace periods with spaces.\n",
        "  line = tf.strings.regex_replace(line, r'\\.', ' ')\n",
        "  tokens = tf.strings.split([line], sep=\" \").values\n",
        "  return tokens, label\n",
        "\n",
        "\n",
        "def get_vocabulary(patterns):\n",
        "  dataset = get_labeled_dataset(patterns)\n",
        "  dataset = dataset.map(tokenize)\n",
        "  dataset = dataset.flat_map(lambda x, y: tf.data.Dataset.from_tensor_slices(x))\n",
        "  dataset = dataset.apply(tf.data.experimental.unique())\n",
        "  dataset = tf.data.Dataset.zip((dataset, tf.data.experimental.Counter()))\n",
        "\n",
        "  vocabulary = {}\n",
        "  for word, index in iter(dataset):\n",
        "    vocabulary[word.numpy()] = index\n",
        "  return vocabulary\n",
        "\n",
        "\n",
        "def get_indexed_dataset(vocabulary, patterns):\n",
        "\n",
        "  def index_and_pad(word_list, label):\n",
        "\n",
        "    def helper(word_list):\n",
        "      result = []\n",
        "      for word, _ in zip(word_list, range(MAX_LEN)):\n",
        "        result.append(vocabulary[word])\n",
        "      return tf.pad(result, [[0, MAX_LEN - len(result)]], 'CONSTANT')\n",
        "\n",
        "    return tf.numpy_function(helper, [word_list], tf.int64), label\n",
        "\n",
        "  dataset = get_labeled_dataset(patterns)\n",
        "  dataset = dataset.map(tokenize)\n",
        "  dataset = dataset.map(index_and_pad)\n",
        "  dataset = dataset.shuffle(10 * BATCH_SIZE)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  return dataset\n",
        "\n",
        "\n",
        "file = 'aclImdb_v1.tar.gz'\n",
        "file = tf.keras.utils.get_file(file, DATA_URL, extract=True)\n",
        "path = os.path.join(os.path.dirname(file), 'aclImdb')\n",
        "\n",
        "train_files = [\n",
        "    os.path.join(path, 'train', label, '*') for label in ['pos', 'neg']\n",
        "]\n",
        "test_files = [\n",
        "    os.path.join(path, 'test', label, '*') for label in ['pos', 'neg']\n",
        "]\n",
        "\n",
        "vocabulary = get_vocabulary(train_files + test_files)\n",
        "print(len(vocabulary))\n",
        "train_data = get_indexed_dataset(vocabulary, train_files)\n",
        "\n",
        "model = get_model(len(vocabulary))\n",
        "model.fit(train_data, epochs=10)\n",
        "\n",
        "test_data = get_indexed_dataset(vocabulary, test_files)\n",
        "model.evaluate(test_data)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-alpha0 in /usr/local/lib/python2.7/dist-packages (2.0.0a0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (2.0.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.7)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (3.7.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0a20190301)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.1.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.11.0)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.0.post1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python2.7/dist-packages (from tensorflow==2.0.0-alpha0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow==2.0.0-alpha0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow==2.0.0-alpha0) (5.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-alpha0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-alpha0) (40.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0) (3.0.1)\n",
            "345418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0321 22:29:00.914769 140374431422336 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/script_ops.py:476: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "49/49 [==============================] - 38s 769ms/step - loss: 0.6856 - accuracy: 0.5904\n",
            "Epoch 2/10\n",
            "49/49 [==============================] - 36s 744ms/step - loss: 0.5781 - accuracy: 0.8380\n",
            "Epoch 3/10\n",
            "49/49 [==============================] - 37s 749ms/step - loss: 0.3396 - accuracy: 0.8825\n",
            "Epoch 4/10\n",
            "49/49 [==============================] - 37s 750ms/step - loss: 0.1999 - accuracy: 0.9308\n",
            "Epoch 5/10\n",
            "49/49 [==============================] - 39s 800ms/step - loss: 0.1140 - accuracy: 0.9659\n",
            "Epoch 6/10\n",
            "49/49 [==============================] - 38s 771ms/step - loss: 0.0593 - accuracy: 0.9877\n",
            "Epoch 7/10\n",
            "49/49 [==============================] - 37s 765ms/step - loss: 0.0296 - accuracy: 0.9963\n",
            "Epoch 8/10\n",
            "49/49 [==============================] - 37s 757ms/step - loss: 0.0152 - accuracy: 0.9990\n",
            "Epoch 9/10\n",
            "49/49 [==============================] - 36s 743ms/step - loss: 0.0085 - accuracy: 0.9998\n",
            "Epoch 10/10\n",
            "49/49 [==============================] - 36s 742ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "     49/Unknown - 24s 496ms/step - loss: 0.3709 - accuracy: 0.8570"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37087987332927935, 0.85696]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    }
  ]
}