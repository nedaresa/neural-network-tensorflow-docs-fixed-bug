{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_nWetWWd_ns"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "2pHVBk_seED1"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jo5PziEC4hWs"
   },
   "source": [
    "# Neural Style Transfer with tf.keras\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/Choiuijin1125/docs/blob/Ko-tutorials-eager/site/ko/tutorials/eager/_style_transfer.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 이 문서는 텐서플로 커뮤니티에서 번역했습니다. 커뮤니티 번역 활동의 특성상 정확한 번역과 최신 내용을 반영하기 위해 노력함에도\n",
    "불구하고 [공식 영문 문서](https://www.tensorflow.org/?hl=en)의 내용과 일치하지 않을 수 있습니다.\n",
    "이 번역에 개선할 부분이 있다면\n",
    "[tensorflow/docs](https://github.com/tensorflow/docs) 깃헙 저장소로 풀 리퀘스트를 보내주시기 바랍니다.\n",
    "문서 번역이나 리뷰에 지원하려면 [이 양식](https://bit.ly/tf-translate)을\n",
    "작성하거나\n",
    "[docs@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs)로\n",
    "메일을 보내주시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDyGj8DmXCJI"
   },
   "source": [
    "## Overview\n",
    "\n",
    "이번 튜토리얼에서는 딥러닝을 사용하여 다른 스타일의 이미지로 이미지를 구성하는법을 배워보겠습니다(피카소나 반 고흐처럼 그리기를 희망하시나요?). 이 기법은 **neural style transfer**로 알려져있습니다. 이 기법은 [Leon A. Gatys' paper, A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)에 잘 기술되어 있습니다. 꼭 읽어보셔야합니다.\n",
    "\n",
    "하지만 무엇이 neural style transfer 일까요?\n",
    "\n",
    "Neural style transfer은 3가지의 이미지-**Content** 이미지, **Style Reference** 이미지(유명한 작가의 삽화 같은), 그리고 여러분이 변경하기 원하는 스타일의 **Input** 이미지-를 사용하여, 위 이미지들을 섞어 content 이미지로 content가 변형 된 것처럼 보이고, style reference 이미지로 색칠된 input 이미지를 생성하는 최적화 기술입니다.\n",
    "\n",
    "예를 들어, 이 거북이 이미지와, Katsushika Hokusai 작가의 *The Great Wave off Kanagawa*:를 사용해봅시다.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/models/blob/master/research/nst_blogpost/Green_Sea_Turtle_grazing_seagrass.jpg?raw=1\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "<img src=\"https://github.com/tensorflow/models/blob/master/research/nst_blogpost/The_Great_Wave_off_Kanagawa.jpg?raw=1\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "[Image of Green Sea Turtle](https://commons.wikimedia.org/wiki/File:Green_Sea_Turtle_grazing_seagrass.jpg)\n",
    "-By P.Lindgren [CC BY-SA 3.0  (https://creativecommons.org/licenses/by-sa/3.0)], from Wikimedia Common\n",
    "\n",
    "\n",
    "만약 hokusai 작가가 오직 위 스타일로 이 거북이를 그린다면 어떻게 생겼을까요? 위와 비슷할까요?\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/models/blob/master/research/nst_blogpost/wave_turtle.png?raw=1\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "이것은 마법일까요 아니면 그저 딥러닝일까요? 다행스럽게도, 이것은 어떠한 마법도 포함하고있지 않습니다. 스타일 변환은 신경망의 기능과 내부표현을 보여주는 재밌고 흥미로운 기술입니다.\n",
    "\n",
    "Neural style transfer의 원리는 얼마나 두 이미지의 내용(content)의 차이가 있는지를 표현하는 함수인, $L_{content}$, 두 이미지의 스타일 차이가 있는지를 표현하는 함수인, $L_{style}$, 위 두가지 거리 함수를 정의하는 것입니다. 원하는 style 이미지, content 이미지 그리고 input 이미지(content 이미지로 초기화 된) 이 3가지의 이미지가 주어졌을 때, content 이미지의 content와 스타일 이미지 스타일의 거리를 최소화하기 위한 input 이미지의 변형을 시도할 것입니다. \n",
    "\n",
    "요약하자면, 기본 input 이미지, 일치시키려는 content 이미지 및 style 이미지를 가져와, 역전파를 이용하여 content와 style 거리(손실)함수를 최소화시켜 content 이미지의 content와 style 이미지의 스타일이 매치된 이미지를 생성할 것입니다. \n",
    "\n",
    "### 적용되는 특별개념:\n",
    "이 과정에서, 우리는 실제 경험을 쌓고 아래의 개념을 중심으로 직관력을 개발할 것입니다.\n",
    "\n",
    "* **즉시실행(Eager Execution)** - 연산을 즉각적으로 평가하는 Tensorflow의 명령형 프로그래밍 환경 사용\n",
    "  * [Learn more about eager execution](https://www.tensorflow.org/programmers_guide/eager)\n",
    "  * [See it in action](https://www.tensorflow.org/get_started/eager)\n",
    "* ** 모델 정의를 위한 [Functional API](https://keras.io/getting-started/functional-api-guide/) 사용** - 우리는 Functional API를 사용하여 필요한 중간 활성화에 대한 액세스를 제공할 모델의 부분집합을 구성할 것입니다.\n",
    "* **사전학습 모델의 레버리징 특정맵(Leveraging feature maps of a pretrained model)** - 사전학습된 모델과 그 모델의 특징맵에 대해 배우겠습니다.\n",
    "* **사용자 지정 훈련 루프 만들기(Create custom training loops)** - 입력변수와 관련하여 주어진 손실함수를 최소화하기 위해 옵티마이저(optimizer)를 설정하는법을 살펴 보겠습니다.\n",
    "\n",
    "### 우리는 스타일 변형을 수행하기 위해 일반적인 단계들을 진행할것입니다.\n",
    "\n",
    "1. 데이터 시각화(Visualize data)\n",
    "2. 기본 전처리 및 데이터 준비(Basic Preprocessing/preparing our data)\n",
    "3. 손실함수 설정(Set up loss functions)\n",
    "4. 모델 생성(Create model)\n",
    "5. 손실함수 최적화(Optimize for loss function)\n",
    "\n",
    "**참고:** -\t이 게시글은 기본 머신러닝 개념에 익숙한 중간사용자를 대상으로합니다. 이 게시글을 최대한 활용하려면, 아래 사이트를 참조하세요.\n",
    "* [Gatys' paper](https://arxiv.org/abs/1508.06576) - we'll explain along the way, but the paper will provide a more thorough understanding of the task\n",
    "* [Understand reducing loss with gradient descent](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent)\n",
    "\n",
    "**예상시간**: 30분\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8ajP_u73s6m"
   },
   "source": [
    "## 설정\n",
    "\n",
    "### 다운로드 이미지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "riWE_b8k3s6o"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "img_dir = '/tmp/nst'\n",
    "if not os.path.exists(img_dir):\n",
    "    os.makedirs(img_dir)\n",
    "!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/d/d7/Green_Sea_Turtle_grazing_seagrass.jpg\n",
    "!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/0/0a/The_Great_Wave_off_Kanagawa.jpg\n",
    "!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/b/b4/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg\n",
    "!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/0/00/Tuebingen_Neckarfront.jpg\n",
    "!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/6/68/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg\n",
    "!wget --quiet -P /tmp/nst/ https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eqxUicSPUOP6"
   },
   "source": [
    "### 모듈 구성 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sc1OLbOWhPCO"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (10,10)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYEjlrYk3s6w"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import image as kp_image\n",
    "from tensorflow.python.keras import models \n",
    "from tensorflow.python.keras import losses\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7sjDODq67HQ"
   },
   "source": [
    "우리는 [즉시실행(eager execution)](https://www.tensorflow.org/guide/eager)을 활성화하여 시작할 것입니다.\n",
    "즉시실행(eager execution)은 가장 명확하고, 읽기 쉬운 방식으로 작업을 할 수 있게 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfjsSAtNrqQx"
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOiGrIV1iERH"
   },
   "outputs": [],
   "source": [
    "# 일부 전역변수를 여기에 설정하세요.\n",
    "content_path = '/tmp/nst/Green_Sea_Turtle_grazing_seagrass.jpg'\n",
    "style_path = '/tmp/nst/The_Great_Wave_off_Kanagawa.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xE4Yt8nArTeR"
   },
   "source": [
    "## 입력 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TLljcwv5qZs"
   },
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "  max_dim = 512\n",
    "  img = Image.open(path_to_img)\n",
    "  long = max(img.size)\n",
    "  scale = max_dim/long\n",
    "  img = img.resize((round(img.size[0]*scale), round(img.size[1]*scale)), Image.ANTIALIAS)\n",
    "  \n",
    "  img = kp_image.img_to_array(img)\n",
    "  \n",
    "  # 배치 차원(batch dimension)을 갖기 위해 이미지 배열을 broadcast할 필요가 있습니다.\n",
    "  img = np.expand_dims(img, axis=0)\n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vupl0CI18aAG"
   },
   "outputs": [],
   "source": [
    "def imshow(img, title=None):\n",
    "  # 배치 차원(batch dimension) 제거\n",
    "  out = np.squeeze(img, axis=0)\n",
    "  # 디스플레이를 위한 정규화\n",
    "  out = out.astype('uint8')\n",
    "  plt.imshow(out)\n",
    "  if title is not None:\n",
    "    plt.title(title)\n",
    "  plt.imshow(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2yAlRzJZrWM3"
   },
   "source": [
    "여기 input content와 style 이미지가 있습니다. 우리는 content 이미지의 content 와 style 이미지의 style로 구성된 이미지를 생성하기를 원합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UWQmeEaiKkP"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "content = load_img(content_path).astype('uint8')\n",
    "style = load_img(style_path).astype('uint8')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(content, 'Content Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(style, 'Style Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7qMVNvEsK-_D"
   },
   "source": [
    "## 데이터 준비\n",
    "우리의 이미지를 쉽게 전처리하고 적재하기위한 메소드(method)를 만들어봅시다. 우리는 VGG 학습 프로세스에 따라 예상되는 것과 동일한 전처리 과정을 수행할것입니다. VGG 네트워크들은 각각의 채널이 `평균 = [103.939, 116.779, 123.68]` 및 BGR 채널들로 정규화되어 있는 이미지로 훈련됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hGwmTwJNmv2a"
   },
   "outputs": [],
   "source": [
    "def load_and_process_img(path_to_img):\n",
    "  img = load_img(path_to_img)\n",
    "  img = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "  return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCgooqs6tAka"
   },
   "source": [
    "최적화의 결과들을 보기 위해 우리는 역전처리 과정을 수행 할 필요가 있습니다. 더욱이, 최적화 된 이미지의 값이 $- \\infty$ and $\\infty$사이의 어느값도 가질 수 있기 때문에, 0-255사이의 값으로 고정해주여야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjzlKRQRs_y2"
   },
   "outputs": [],
   "source": [
    "def deprocess_img(processed_img):\n",
    "  x = processed_img.copy()\n",
    "  if len(x.shape) == 4:\n",
    "    x = np.squeeze(x, 0)\n",
    "  assert len(x.shape) == 3, (\"Input to deprocess image must be an image of \"\n",
    "                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n",
    "  if len(x.shape) != 3:\n",
    "    raise ValueError(\"Invalid input to deprocessing image\")\n",
    "  \n",
    "  # 역전처리 과정 수행\n",
    "  x[:, :, 0] += 103.939\n",
    "  x[:, :, 1] += 116.779\n",
    "  x[:, :, 2] += 123.68\n",
    "  x = x[:, :, ::-1]\n",
    "\n",
    "  x = np.clip(x, 0, 255).astype('uint8')\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GEwZ7FlwrjoZ"
   },
   "source": [
    "### content와 style 표상 정의\n",
    "이미지의 content와 style 표상을 얻기위해, 우리 모델의 중간층들을 살펴볼 것입니다. 우리가 모델을 더 깊게 살펴볼 수록, 중간층들은 더 고차원적 특징들을 나타냅니다. 이번 경우, 우리는 사전학습 된 이미지 분류 네트워크인 VGG19 네트워크의 구조를 사용할 것입니다. 이 중간층들은 우리 이미지에서 content와 style의 표상을 정의할 필요가 있습니다. Input 이미지의 경우, 이 중간층들에서 content와 style에 해당하는 타겟 표상들을 매치하기 위해 시도할 것입니다. \n",
    "\n",
    "#### 왜 중간층이 필요한가?\n",
    "\n",
    "You may be wondering why these intermediate outputs within our pretrained image classification network allow us to define style and content representations. At a high level, this phenomenon can be explained by the fact that in order for a network to perform image classification (which our network has been trained to do), it must understand the image. This involves taking the raw image as input pixels and building an internal representation through transformations that turn the raw image pixels into a complex understanding of the features present within the image. This is also partly why convolutional neural networks are able to generalize well: they’re able to capture the invariances and defining features within classes (e.g., cats vs. dogs) that are agnostic to background noise and other nuisances. Thus, somewhere between where the raw image is fed in and the classification label is output, the model serves as a complex feature extractor; hence by accessing intermediate layers, we’re able to describe the content and style of input images. \n",
    "\n",
    "\n",
    "Specifically we’ll pull out these intermediate layers from our network: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N4-8eUp_Kc-j"
   },
   "outputs": [],
   "source": [
    "# 특징맵들을 가져올 Content layer\n",
    "content_layers = ['block5_conv2'] \n",
    "\n",
    "# 우리가 관심을 갖는 Style layer\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1', \n",
    "                'block4_conv1', \n",
    "                'block5_conv1'\n",
    "               ]\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jt3i3RRrJiOX"
   },
   "source": [
    "## Build the Model \n",
    "In this case, we load [VGG19](https://keras.io/applications/#vgg19), and feed in our input tensor to the model. This will allow us to extract the feature maps (and subsequently the content and style representations) of the content, style, and generated images.\n",
    "\n",
    "We use VGG19, as suggested in the paper. In addition, since VGG19 is a relatively simple model (compared with ResNet, Inception, etc) the feature maps actually work better for style transfer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9AnzEUU6hhx"
   },
   "source": [
    "In order to access the intermediate layers corresponding to our style and content feature maps, we get the corresponding outputs and using the Keras [**Functional API**](https://keras.io/getting-started/functional-api-guide/), we define our model with the desired output activations. \n",
    "\n",
    "With the Functional API defining a model simply involves defining the input and output: \n",
    "\n",
    "`model = Model(inputs, outputs)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfec6MuMAbPx"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  \"\"\" Creates our model with access to intermediate layers. \n",
    "  \n",
    "  This function will load the VGG19 model and access the intermediate layers. \n",
    "  These layers will then be used to create a new model that will take input image\n",
    "  and return the outputs from these intermediate layers from the VGG model. \n",
    "  \n",
    "  Returns:\n",
    "    returns a keras model that takes image inputs and outputs the style and \n",
    "      content intermediate layers. \n",
    "  \"\"\"\n",
    "  # Load our model. We load pretrained VGG, trained on imagenet data\n",
    "  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
    "  vgg.trainable = False\n",
    "  # Get output layers corresponding to style and content layers \n",
    "  style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
    "  content_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
    "  model_outputs = style_outputs + content_outputs\n",
    "  # Build model \n",
    "  return models.Model(vgg.input, model_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kl6eFGa7-OtV"
   },
   "source": [
    "In the above code snippet, we’ll load our pretrained image classification network. Then we grab the layers of interest as we defined earlier. Then we define a Model by setting the model’s inputs to an image and the outputs to the outputs of the style and content layers. In other words, we created a model that will take an input image and output the content and style intermediate layers! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJdYvJTZ4bdS"
   },
   "source": [
    "## Define and create our loss functions (content and style distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2Hcepii7_qh"
   },
   "source": [
    "### Content Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FvH-gwXi4nq"
   },
   "source": [
    "Our content loss definition is actually quite simple. We’ll pass the network both the desired content image and our base input image. This will return the intermediate layer outputs (from the layers defined above) from our model. Then we simply take the euclidean distance between the two intermediate representations of those images.  \n",
    "\n",
    "More formally, content loss is a function that describes the distance of content from our output image $x$ and our content image, $p$. Let $C_{nn}$ be a pre-trained deep convolutional neural network. Again, in this case we use [VGG19](https://keras.io/applications/#vgg19). Let $X$ be any image, then $C_{nn}(X)$ is the network fed by X. Let $F^l_{ij}(x) \\in C_{nn}(x)$ and $P^l_{ij}(p) \\in C_{nn}(p)$ describe the respective intermediate feature representation of the network with inputs $x$ and $p$ at layer $l$. Then we describe the content distance (loss) formally as: $$L^l_{content}(p, x) = \\sum_{i, j} (F^l_{ij}(x) - P^l_{ij}(p))^2$$\n",
    "\n",
    "We perform backpropagation in the usual way such that we minimize this content loss. We thus change the initial image until it generates a similar response in a certain layer (defined in content_layer) as the original content image.\n",
    "\n",
    "This can be implemented quite simply. Again it will take as input the feature maps at a layer L in a network fed by x, our input image, and p, our content image, and return the content distance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6KsbqPA8J9DY"
   },
   "source": [
    "### Computing content loss\n",
    "We will actually add our content losses at each desired layer. This way, each iteration when we feed our input image through the model (which in eager is simply `model(input_image)`!) all the content losses through the model will be properly compute and because we are executing eagerly, all the gradients will be computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2mf7JwRMkCd"
   },
   "outputs": [],
   "source": [
    "def get_content_loss(base_content, target):\n",
    "  return tf.reduce_mean(tf.square(base_content - target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lGUfttK9F8d5"
   },
   "source": [
    "## Style Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6XtkGK_YGD1"
   },
   "source": [
    "Computing style loss is a bit more involved, but follows the same principle, this time feeding our network the base input image and the style image. However, instead of comparing the raw intermediate outputs of the base input image and the style image, we instead compare the Gram matrices of the two outputs. \n",
    "\n",
    "Mathematically, we describe the style loss of the base input image, $x$, and the style image, $a$, as the distance between the style representation (the gram matrices) of these images. We describe the style representation of an image as the correlation between different filter responses given by the Gram matrix  $G^l$, where $G^l_{ij}$ is the inner product between the vectorized feature map $i$ and $j$ in layer $l$. We can see that $G^l_{ij}$ generated over the feature map for a given image represents the correlation between feature maps $i$ and $j$. \n",
    "\n",
    "To generate a style for our base input image, we perform gradient descent from the content image to transform it into an image that matches the style representation of the original image. We do so by minimizing the mean squared distance between the feature correlation map of the style image and the input image. The contribution of each layer to the total style loss is described by\n",
    "$$E_l = \\frac{1}{4N_l^2M_l^2} \\sum_{i,j}(G^l_{ij} - A^l_{ij})^2$$\n",
    "\n",
    "where $G^l_{ij}$ and $A^l_{ij}$ are the respective style representation in layer $l$ of $x$ and $a$. $N_l$ describes the number of feature maps, each of size $M_l = height * width$. Thus, the total style loss across each layer is \n",
    "$$L_{style}(a, x) = \\sum_{l \\in L} w_l E_l$$\n",
    "where we weight the contribution of each layer's loss by some factor $w_l$. In our case, we weight each layer equally ($w_l =\\frac{1}{|L|}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F21Hm61yLKk5"
   },
   "source": [
    "### Computing style loss\n",
    "Again, we implement our loss as a distance metric . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N7MOqwKLLke8"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(input_tensor):\n",
    "  # We make the image channels first \n",
    "  channels = int(input_tensor.shape[-1])\n",
    "  a = tf.reshape(input_tensor, [-1, channels])\n",
    "  n = tf.shape(a)[0]\n",
    "  gram = tf.matmul(a, a, transpose_a=True)\n",
    "  return gram / tf.cast(n, tf.float32)\n",
    "\n",
    "def get_style_loss(base_style, gram_target):\n",
    "  \"\"\"Expects two images of dimension h, w, c\"\"\"\n",
    "  # height, width, num filters of each layer\n",
    "  # We scale the loss at a given layer by the size of the feature map and the number of filters\n",
    "  height, width, channels = base_style.get_shape().as_list()\n",
    "  gram_style = gram_matrix(base_style)\n",
    "  \n",
    "  return tf.reduce_mean(tf.square(gram_style - gram_target))# / (4. * (channels ** 2) * (width * height) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pXIUX6czZABh"
   },
   "source": [
    "## Apply style transfer to our images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9r8Lyjb_m0u"
   },
   "source": [
    "### Run Gradient Descent \n",
    "If you aren't familiar with gradient descent/backpropagation or need a refresher, you should definitely check out this [awesome resource](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent).\n",
    "\n",
    "In this case, we use the [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)* optimizer in order to minimize our loss. We iteratively update our output image such that it minimizes our loss: we don't update the weights associated with our network, but instead we train our input image to minimize loss. In order to do this, we must know how we calculate our loss and gradients. \n",
    "\n",
    "\\* Note that L-BFGS, which if you are familiar with this algorithm is recommended, isn’t used in this tutorial because a primary motivation behind this tutorial was to illustrate best practices with eager execution, and, by using Adam, we can demonstrate the autograd/gradient tape functionality with custom training loops.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kGzV6LTp4CU"
   },
   "source": [
    "We’ll define a little helper function that will load our content and style image, feed them forward through our network, which will then output the content and style feature representations from our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O-lj5LxgtmnI"
   },
   "outputs": [],
   "source": [
    "def get_feature_representations(model, content_path, style_path):\n",
    "  \"\"\"Helper function to compute our content and style feature representations.\n",
    "\n",
    "  This function will simply load and preprocess both the content and style \n",
    "  images from their path. Then it will feed them through the network to obtain\n",
    "  the outputs of the intermediate layers. \n",
    "  \n",
    "  Arguments:\n",
    "    model: The model that we are using.\n",
    "    content_path: The path to the content image.\n",
    "    style_path: The path to the style image\n",
    "    \n",
    "  Returns:\n",
    "    returns the style features and the content features. \n",
    "  \"\"\"\n",
    "  # Load our images in \n",
    "  content_image = load_and_process_img(content_path)\n",
    "  style_image = load_and_process_img(style_path)\n",
    "  \n",
    "  # batch compute content and style features\n",
    "  style_outputs = model(style_image)\n",
    "  content_outputs = model(content_image)\n",
    "  \n",
    "  \n",
    "  # Get the style and content feature representations from our model  \n",
    "  style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]\n",
    "  content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]\n",
    "  return style_features, content_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DopXw7-lFHa"
   },
   "source": [
    "### Computing the loss and gradients\n",
    "Here we use [**tf.GradientTape**](https://www.tensorflow.org/programmers_guide/eager#computing_gradients) to compute the gradient. It allows us to take advantage of the automatic differentiation available by tracing operations for computing the gradient later. It records the operations during the forward pass and then is able to compute the gradient of our loss function with respect to our input image for the backwards pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVDhSo8iJunf"
   },
   "outputs": [],
   "source": [
    "def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\n",
    "  \"\"\"This function will compute the loss total loss.\n",
    "  \n",
    "  Arguments:\n",
    "    model: The model that will give us access to the intermediate layers\n",
    "    loss_weights: The weights of each contribution of each loss function. \n",
    "      (style weight, content weight, and total variation weight)\n",
    "    init_image: Our initial base image. This image is what we are updating with \n",
    "      our optimization process. We apply the gradients wrt the loss we are \n",
    "      calculating to this image.\n",
    "    gram_style_features: Precomputed gram matrices corresponding to the \n",
    "      defined style layers of interest.\n",
    "    content_features: Precomputed outputs from defined content layers of \n",
    "      interest.\n",
    "      \n",
    "  Returns:\n",
    "    returns the total loss, style loss, content loss, and total variational loss\n",
    "  \"\"\"\n",
    "  style_weight, content_weight = loss_weights\n",
    "  \n",
    "  # Feed our init image through our model. This will give us the content and \n",
    "  # style representations at our desired layers. Since we're using eager\n",
    "  # our model is callable just like any other function!\n",
    "  model_outputs = model(init_image)\n",
    "  \n",
    "  style_output_features = model_outputs[:num_style_layers]\n",
    "  content_output_features = model_outputs[num_style_layers:]\n",
    "  \n",
    "  style_score = 0\n",
    "  content_score = 0\n",
    "\n",
    "  # Accumulate style losses from all layers\n",
    "  # Here, we equally weight each contribution of each loss layer\n",
    "  weight_per_style_layer = 1.0 / float(num_style_layers)\n",
    "  for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
    "    style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)\n",
    "    \n",
    "  # Accumulate content losses from all layers \n",
    "  weight_per_content_layer = 1.0 / float(num_content_layers)\n",
    "  for target_content, comb_content in zip(content_features, content_output_features):\n",
    "    content_score += weight_per_content_layer* get_content_loss(comb_content[0], target_content)\n",
    "  \n",
    "  style_score *= style_weight\n",
    "  content_score *= content_weight\n",
    "\n",
    "  # Get total loss\n",
    "  loss = style_score + content_score \n",
    "  return loss, style_score, content_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5XTvbP6nJQa"
   },
   "source": [
    "Then computing the gradients is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fwzYeOqOUH9_"
   },
   "outputs": [],
   "source": [
    "def compute_grads(cfg):\n",
    "  with tf.GradientTape() as tape: \n",
    "    all_loss = compute_loss(**cfg)\n",
    "  # Compute gradients wrt input image\n",
    "  total_loss = all_loss[0]\n",
    "  return tape.gradient(total_loss, cfg['init_image']), all_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9yKu2PLlBIE"
   },
   "source": [
    "### Optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pj_enNo6tACQ"
   },
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "\n",
    "def run_style_transfer(content_path, \n",
    "                       style_path,\n",
    "                       num_iterations=1000,\n",
    "                       content_weight=1e3, \n",
    "                       style_weight=1e-2): \n",
    "  # We don't need to (or want to) train any layers of our model, so we set their\n",
    "  # trainable to false. \n",
    "  model = get_model() \n",
    "  for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "  \n",
    "  # Get the style and content feature representations (from our specified intermediate layers) \n",
    "  style_features, content_features = get_feature_representations(model, content_path, style_path)\n",
    "  gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
    "  \n",
    "  # Set initial image\n",
    "  init_image = load_and_process_img(content_path)\n",
    "  init_image = tfe.Variable(init_image, dtype=tf.float32)\n",
    "  # Create our optimizer\n",
    "  opt = tf.train.AdamOptimizer(learning_rate=5, beta1=0.99, epsilon=1e-1)\n",
    "\n",
    "  # For displaying intermediate images \n",
    "  iter_count = 1\n",
    "  \n",
    "  # Store our best result\n",
    "  best_loss, best_img = float('inf'), None\n",
    "  \n",
    "  # Create a nice config \n",
    "  loss_weights = (style_weight, content_weight)\n",
    "  cfg = {\n",
    "      'model': model,\n",
    "      'loss_weights': loss_weights,\n",
    "      'init_image': init_image,\n",
    "      'gram_style_features': gram_style_features,\n",
    "      'content_features': content_features\n",
    "  }\n",
    "    \n",
    "  # For displaying\n",
    "  num_rows = 2\n",
    "  num_cols = 5\n",
    "  display_interval = num_iterations/(num_rows*num_cols)\n",
    "  start_time = time.time()\n",
    "  global_start = time.time()\n",
    "  \n",
    "  norm_means = np.array([103.939, 116.779, 123.68])\n",
    "  min_vals = -norm_means\n",
    "  max_vals = 255 - norm_means   \n",
    "  \n",
    "  imgs = []\n",
    "  for i in range(num_iterations):\n",
    "    grads, all_loss = compute_grads(cfg)\n",
    "    loss, style_score, content_score = all_loss\n",
    "    opt.apply_gradients([(grads, init_image)])\n",
    "    clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n",
    "    init_image.assign(clipped)\n",
    "    end_time = time.time() \n",
    "    \n",
    "    if loss < best_loss:\n",
    "      # Update best loss and best image from total loss. \n",
    "      best_loss = loss\n",
    "      best_img = deprocess_img(init_image.numpy())\n",
    "\n",
    "    if i % display_interval== 0:\n",
    "      start_time = time.time()\n",
    "      \n",
    "      # Use the .numpy() method to get the concrete numpy array\n",
    "      plot_img = init_image.numpy()\n",
    "      plot_img = deprocess_img(plot_img)\n",
    "      imgs.append(plot_img)\n",
    "      IPython.display.clear_output(wait=True)\n",
    "      IPython.display.display_png(Image.fromarray(plot_img))\n",
    "      print('Iteration: {}'.format(i))        \n",
    "      print('Total loss: {:.4e}, ' \n",
    "            'style loss: {:.4e}, '\n",
    "            'content loss: {:.4e}, '\n",
    "            'time: {:.4f}s'.format(loss, style_score, content_score, time.time() - start_time))\n",
    "  print('Total time: {:.4f}s'.format(time.time() - global_start))\n",
    "  IPython.display.clear_output(wait=True)\n",
    "  plt.figure(figsize=(14,4))\n",
    "  for i,img in enumerate(imgs):\n",
    "      plt.subplot(num_rows,num_cols,i+1)\n",
    "      plt.imshow(img)\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "      \n",
    "  return best_img, best_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSVMx4burydi"
   },
   "outputs": [],
   "source": [
    "best, best_loss = run_style_transfer(content_path, \n",
    "                                     style_path, num_iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzJTObpsO3TZ"
   },
   "outputs": [],
   "source": [
    "Image.fromarray(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCXQ9vSnQbDy"
   },
   "source": [
    "To download the image from Colab uncomment the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SSH6OpyyQn7w"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#files.download('wave_turtle.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LwiZfCW0AZwt"
   },
   "source": [
    "## Visualize outputs\n",
    "We \"deprocess\" the output image in order to remove the processing that was applied to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqTQN1PjulV9"
   },
   "outputs": [],
   "source": [
    "def show_results(best_img, content_path, style_path, show_large_final=True):\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  content = load_img(content_path) \n",
    "  style = load_img(style_path)\n",
    "\n",
    "  plt.subplot(1, 2, 1)\n",
    "  imshow(content, 'Content Image')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  imshow(style, 'Style Image')\n",
    "\n",
    "  if show_large_final: \n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    plt.imshow(best_img)\n",
    "    plt.title('Output Image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6d6O50Yvs6a"
   },
   "outputs": [],
   "source": [
    "show_results(best, content_path, style_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyGMmWh2Pss8"
   },
   "source": [
    "## Try it on other images\n",
    "Image of Tuebingen \n",
    "\n",
    "Photo By: Andreas Praefcke [GFDL (http://www.gnu.org/copyleft/fdl.html) or CC BY 3.0  (https://creativecommons.org/licenses/by/3.0)], from Wikimedia Commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x2TePU39k9lb"
   },
   "source": [
    "### Starry night + Tuebingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ES9dC6ZyJBD2"
   },
   "outputs": [],
   "source": [
    "best_starry_night, best_loss = run_style_transfer('/tmp/nst/Tuebingen_Neckarfront.jpg',\n",
    "                                                  '/tmp/nst/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8w8WLkKvzXu"
   },
   "outputs": [],
   "source": [
    "show_results(best_starry_night, '/tmp/nst/Tuebingen_Neckarfront.jpg',\n",
    "             '/tmp/nst/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QcXwvViek4Br"
   },
   "source": [
    "### Pillars of Creation + Tuebingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJ3u2U-gGmgP"
   },
   "outputs": [],
   "source": [
    "best_poc_tubingen, best_loss = run_style_transfer('/tmp/nst/Tuebingen_Neckarfront.jpg', \n",
    "                                                  '/tmp/nst/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pQUq3KxpGv2O"
   },
   "outputs": [],
   "source": [
    "show_results(best_poc_tubingen, \n",
    "             '/tmp/nst/Tuebingen_Neckarfront.jpg',\n",
    "             '/tmp/nst/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bTZdTOdW3s8H"
   },
   "source": [
    "### Kandinsky Composition 7 + Tuebingen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bt9mbQfl7exl"
   },
   "outputs": [],
   "source": [
    "best_kandinsky_tubingen, best_loss = run_style_transfer('/tmp/nst/Tuebingen_Neckarfront.jpg', \n",
    "                                                  '/tmp/nst/Vassily_Kandinsky,_1913_-_Composition_7.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qnz8HeXSXg6P"
   },
   "outputs": [],
   "source": [
    "show_results(best_kandinsky_tubingen, \n",
    "             '/tmp/nst/Tuebingen_Neckarfront.jpg',\n",
    "             '/tmp/nst/Vassily_Kandinsky,_1913_-_Composition_7.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cg68lW2A3s8N"
   },
   "source": [
    "### Pillars of Creation + Sea Turtle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dl0DUot_bFST"
   },
   "outputs": [],
   "source": [
    "best_poc_turtle, best_loss = run_style_transfer('/tmp/nst/Green_Sea_Turtle_grazing_seagrass.jpg', \n",
    "                                                  '/tmp/nst/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzJfE0I1bQn8"
   },
   "outputs": [],
   "source": [
    "show_results(best_poc_turtle, \n",
    "             '/tmp/nst/Green_Sea_Turtle_grazing_seagrass.jpg',\n",
    "             '/tmp/nst/Pillars_of_creation_2014_HST_WFC3-UVIS_full-res_denoised.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sElaeNX-4Vnc"
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What we covered:\n",
    "\n",
    "* We built several different loss functions and used backpropagation to transform our input image in order to minimize these losses\n",
    "  * In order to do this we had to load in a **pretrained model** and use its learned feature maps to describe the content and style representation of our images.\n",
    "    * Our main loss functions were primarily computing the distance in terms of these different representations\n",
    "* We implemented this with a custom model and **eager execution**\n",
    "  * We built our custom model with the Functional API \n",
    "  * Eager execution allows us to dynamically work with tensors, using a natural python control flow\n",
    "  * We manipulated tensors directly, which makes debugging and working with tensors easier. \n",
    "* We iteratively updated our image by applying our optimizers update rules using **tf.gradient**. The optimizer minimized a given loss with respect to our input image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U-y02GWonqnD"
   },
   "source": [
    "\n",
    "**[Image of Tuebingen](https://commons.wikimedia.org/wiki/File:Tuebingen_Neckarfront.jpg)** \n",
    "Photo By: Andreas Praefcke [GFDL (http://www.gnu.org/copyleft/fdl.html) or CC BY 3.0  (https://creativecommons.org/licenses/by/3.0)], from Wikimedia Commons\n",
    "\n",
    "**[Image of Green Sea Turtle](https://commons.wikimedia.org/wiki/File:Green_Sea_Turtle_grazing_seagrass.jpg)**\n",
    "By P.Lindgren [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)], from Wikimedia Commons\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpUD9W6ZkeyM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Style Transfer with Eager Execution",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
